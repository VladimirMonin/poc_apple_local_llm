# –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è Qwen3-VL-4B-Instruct –Ω–∞ Apple Silicon (M2, 8GB RAM)

**–î–∞—Ç–∞:** 7 –¥–µ–∫–∞–±—Ä—è 2025 –≥.  
**–ó–∞–¥–∞—á–∞:** –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è Vision-Language –º–æ–¥–µ–ª–∏ Qwen3-VL-4B —á–µ—Ä–µ–∑ MLX Framework  
**–¶–µ–ª—å:** –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ AI –Ω–∞ –±—é–¥–∂–µ—Ç–Ω–æ–º Mac (8GB RAM)  
**–†–µ–∑—É–ª—å—Ç–∞—Ç:** ‚úÖ **–£–°–ü–ï–•** (—Å –≤–∞–∂–Ω—ã–º–∏ –æ–≥–æ–≤–æ—Ä–∫–∞–º–∏)

---

## –û–≥–ª–∞–≤–ª–µ–Ω–∏–µ

1. [–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏ –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ](#1-–∫–æ–Ω—Ç–µ–∫—Å—Ç-–∏-–∏—Å—Ö–æ–¥–Ω—ã–µ-–¥–∞–Ω–Ω—ã–µ)
2. [–•—Ä–æ–Ω–æ–ª–æ–≥–∏—è –ø—Ä–æ–±–ª–µ–º –∏ —Ä–µ—à–µ–Ω–∏–π](#2-—Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—è-–ø—Ä–æ–±–ª–µ–º-–∏-—Ä–µ—à–µ–Ω–∏–π)
3. [–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –∞—Ä—Ö–µ–æ–ª–æ–≥–∏—è](#3-—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è-–∞—Ä—Ö–µ–æ–ª–æ–≥–∏—è)
4. [–§–∏–Ω–∞–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ](#4-—Ñ–∏–Ω–∞–ª—å–Ω–æ–µ-—Ä–µ—à–µ–Ω–∏–µ)
5. [–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏](#5-–∞–Ω–∞–ª–∏–∑-–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏)
6. [–í—ã–≤–æ–¥—ã –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏](#6-–≤—ã–≤–æ–¥—ã-–∏-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏)

---

## 1. –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏ –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ

### 1.1 –ò—Å—Ö–æ–¥–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã

**–ñ–µ–ª–µ–∑–æ:**
- MacBook Air M3 (2023)
- RAM: 8 GB (Unified Memory)
- SSD: 256 GB
- macOS: Sequoia 15.6.1

**–°–æ—Å—Ç–æ—è–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –Ω–∞ –º–æ–º–µ–Ω—Ç –Ω–∞—á–∞–ª–∞:**
- –î–æ—Å—Ç—É–ø–Ω–∞—è RAM: ~3 –ì–ë
- Swap –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è: 1.7 –ì–ë / 3 –ì–ë
- Memory Pressure: **Critical** (–∫—Ä–∞—Å–Ω–∞—è –∑–æ–Ω–∞)
- –°–∂–∞—Ç–∞—è –ø–∞–º—è—Ç—å: ~35 –ì–ë

**Python –æ–∫—Ä—É–∂–µ–Ω–∏–µ:**
- Python 3.11.14
- mlx 0.30.0
- mlx-vlm 0.3.9
- transformers 4.57.3 (–∏–∑–Ω–∞—á–∞–ª—å–Ω–æ)
- torch 2.9.1 (—É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –¥–ª—è –¥—Ä—É–≥–∏—Ö —Ü–µ–ª–µ–π)

### 1.2 –¶–µ–ª–µ–≤–∞—è –º–æ–¥–µ–ª—å

**–°–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑ –¢–ó (poc_plan.md):**
```
–ú–æ–¥–µ–ª—å: mlx-community/Qwen3-VL-4B-Instruct-4bit
–†–∞–∑–º–µ—Ä: ~3.3 –ì–ë (4-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è)
–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: 4B
–û–∂–∏–¥–∞–µ–º–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å: >20 —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫
–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –ø–∞–º—è—Ç–∏: <4 –ì–ë RAM
–ü–æ–¥–¥–µ—Ä–∂–∫–∞: –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è + —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫
```

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏ (–∏–∑ GDR-–æ—Ç—á–µ—Ç–∞):**
- Vision encoder: DeepStack (multi-level vision-language fusion)
- Positional embeddings: Interleaved-MRoPE (multimodal)
- Temporal support: Conv3d layers (video capability)
- Quantization: 4-bit (AWQ/GPTQ)

### 1.3 –ü—Ä–∏–Ω—Ü–∏–ø—ã –ø—Ä–æ–µ–∫—Ç–∞

–ò–∑ `.github/instructions/concept.instructions.md`:

**–ö–†–ò–¢–ò–ß–ù–û:** "–ë–ï–ó —Ç–∞–Ω—Ü–µ–≤ —Å –±—É–±–Ω–æ–º"
- –ò–∑–±–µ–≥–∞—Ç—å –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö workaround'–æ–≤
- –ù–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å PyTorch MPS backend (—Ç–æ–ª—å–∫–æ MLX)
- –ù–µ —Ç—è–Ω—É—Ç—å –ª–∏—à–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
- –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –≤–∞–∂–Ω–µ–µ –≥–∏–±–∫–æ—Å—Ç–∏

---

## 2. –•—Ä–æ–Ω–æ–ª–æ–≥–∏—è –ø—Ä–æ–±–ª–µ–º –∏ —Ä–µ—à–µ–Ω–∏–π

### 2.1 –ü–µ—Ä–≤–∞—è –ø–æ–ø—ã—Ç–∫–∞: –û—à–∏–±–∫–∞ AutoVideoProcessor (18:45 MSK)

**–î–µ–π—Å—Ç–≤–∏–µ:**
–ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞ `04_test_qwen.py` —Å –º–æ–¥–µ–ª—å—é `mlx-community/Qwen3-VL-4B-Instruct-4bit`

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
```
AutoVideoProcessor requires the Torchvision library but it was not found
```

**–ü–µ—Ä–≤–∏—á–Ω–∞—è –≥–∏–ø–æ—Ç–µ–∑–∞:**
–ú–æ–¥–µ–ª—å Qwen3-VL –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≤–∏–¥–µ–æ ‚Üí —Ç—Ä–µ–±—É–µ—Ç AutoVideoProcessor ‚Üí –Ω—É–∂–µ–Ω torchvision

**–ü—Ä–æ–±–ª–µ–º–∞ —Å –≥–∏–ø–æ—Ç–µ–∑–æ–π:**
–ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—Ç –ø—Ä–∏–Ω—Ü–∏–ø—É "–ë–ï–ó —Ç–∞–Ω—Ü–µ–≤ —Å –±—É–±–Ω–æ–º" ‚Äî –∑–∞—á–µ–º —Å—Ç–∞–≤–∏—Ç—å torchvision (~500 –ú–ë + –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ PyTorch) –¥–ª—è —Ä–∞–±–æ—Ç—ã –¢–û–õ–¨–ö–û —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏?

### 2.2 –ì–ª—É–±–æ–∫–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ Brave Search (18:50 - 19:15 MSK)

**–ü–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã:**
1. "mlx-vlm Qwen3-VL torchvision AutoVideoProcessor"
2. "AutoVideoProcessor mlx-vlm Qwen image only"
3. "mlx-vlm Qwen3-VL-4B-Instruct-4bit preprocessor_config"

**–ö–ª—é—á–µ–≤—ã–µ –Ω–∞—Ö–æ–¥–∫–∏:**

#### –ù–∞—Ö–æ–¥–∫–∞ 1: Issue #209 –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ mlx-vlm
- **–î–∞—Ç–∞:** –ó–∞–∫—Ä—ã—Ç –≤ –º–∞—Ä—Ç–µ 2025 –≥–æ–¥–∞
- **–ü—Ä–æ–±–ª–µ–º–∞:** "Unrecognized image processor in Qwen2.5-VL-7B-Instruct-4bit"
- **–ü—Ä–∏—á–∏–Ω–∞:** HuggingFace Transformers 4.49+ —É–¥–∞–ª–∏–ª–∏ –∫–ª–∞—Å—Å `Qwen2.5VLImageProcessor`
- **–û–±—ä—è—Å–Ω–µ–Ω–∏–µ:** –ö–ª–∞—Å—Å –±—ã–ª –∏–¥–µ–Ω—Ç–∏—á–µ–Ω `Qwen2VLImageProcessor`, –ø–æ—ç—Ç–æ–º—É –µ–≥–æ –æ–±—ä–µ–¥–∏–Ω–∏–ª–∏

**–¶–∏—Ç–∞—Ç–∞ –∏–∑ Issue #209 (–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π @Blaizzy, maintainer mlx-vlm):**
> "This is an issue on the transformers side. The Qwen2.5VLImageProcessor class was deleted as it's identical with Qwen2VL. The fix is to either:
> 1. Change "image_processor_type": "Qwen2VLImageProcessor" in preprocessor_config.json
> 2. Or wait till I update all models on the Hub"

#### –ù–∞—Ö–æ–¥–∫–∞ 2: Backwards compatibility –ø–∞—Ç—á –æ—Ç LM Studio
–†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: `lmstudio-ai/mlx-engine`  
–ö–æ–º–º–∏—Ç: `3f248540` (7 –º–∞—Ä—Ç–∞ 2025)

–ü–∞—Ç—á –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç legacy –º–æ–¥–µ–ª–∏ —Å —É—Å—Ç–∞—Ä–µ–≤—à–∏–º–∏ processor —Ç–∏–ø–∞–º–∏, –Ω–æ **–ù–ï –±—ã–ª –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω –≤ mlx-vlm**.

#### –ù–∞—Ö–æ–¥–∫–∞ 3: preprocessor_config.json –¥–ª—è Qwen3-VL
–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–∞ —á–µ—Ä–µ–∑ Brave:
```json
{
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "processor_class": "Qwen3VLProcessor",
  ...
}
```

**–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–ë–õ–ï–ú–ê:**
- `Qwen2VLImageProcessorFast` **–ù–ï –°–£–©–ï–°–¢–í–£–ï–¢** –≤ transformers 4.49+
- `Qwen3VLProcessor` –ø—ã—Ç–∞–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `AutoVideoProcessor` –∫–∞–∫ fallback
- `AutoVideoProcessor` —Ç—Ä–µ–±—É–µ—Ç torchvision

### 2.3 –ü–æ–ø—ã—Ç–∫–∞ 2: –ü–∞—Ç—á preprocessor_config.json (19:20 MSK)

**–ì–∏–ø–æ—Ç–µ–∑–∞:**
–ò–∑–º–µ–Ω–∏—Ç—å `"Qwen2VLImageProcessorFast"` ‚Üí `"Qwen2VLImageProcessor"` –≤ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–º —Ñ–∞–π–ª–µ

**–î–µ–π—Å—Ç–≤–∏—è:**
```bash
# –ü–æ–∏—Å–∫ —Ñ–∞–π–ª–∞
find ~/.cache/huggingface/hub -name "preprocessor_config.json" -path "*Qwen3-VL-4B*"

# –†–µ–∑—É–ª—å—Ç–∞—Ç
/Users/v/.cache/huggingface/hub/models--mlx-community--Qwen3-VL-4B-Instruct-4bit/
  snapshots/2fd8dacbdb8f1e54b8c005f081ec5bf79c56376b/preprocessor_config.json
```

**–ü—Ä–æ–±–ª–µ–º–∞ 1:** –§–∞–π–ª –æ–∫–∞–∑–∞–ª—Å—è —Å–∏–º–ª–∏–Ω–∫–æ–º
**–†–µ—à–µ–Ω–∏–µ:** –ù–∞—à–ª–∏ —Ä–µ–∞–ª—å–Ω—ã–π blob:
```bash
realpath preprocessor_config.json
# ‚Üí /Users/v/.cache/huggingface/hub/.../blobs/2fa6553554f69b4e3a6b742a75264ff9dff90ad0
```

**–ü—Ä–∏–º–µ–Ω–∏–ª–∏ –ø–∞—Ç—á:**
```bash
sed -i.bak 's/"Qwen2VLImageProcessorFast"/"Qwen2VLImageProcessor"/g' [—Ñ–∞–π–ª]
```

**–ü—Ä–æ–≤–µ—Ä–∫–∞:**
```json
"image_processor_type": "Qwen2VLImageProcessor"  // ‚úÖ –ò–∑–º–µ–Ω–µ–Ω–æ
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç —Ç–µ—Å—Ç–∞:**
```
AutoVideoProcessor requires the Torchvision library...
```

**–¢–ê –ñ–ï –û–®–ò–ë–ö–ê!** üò§

### 2.4 –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏—Å—Ö–æ–¥–Ω–∏–∫–æ–≤ mlx-vlm —á–µ—Ä–µ–∑ GitHub (19:25 - 19:45 MSK)

**–¶–µ–ª—å:** –ü–æ–Ω—è—Ç—å, –ø–æ—á–µ–º—É –ø–∞—Ç—á –Ω–µ –ø–æ–º–æ–≥

**–ú–µ—Ç–æ–¥:** –ü–æ–∏—Å–∫ –ø–æ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—é `Blaizzy/mlx-vlm` —á–µ—Ä–µ–∑ GitHub API

**–ó–∞–ø—Ä–æ—Å 1:** –ü–æ–∏—Å–∫ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤
```python
grep "AutoProcessor.register.*qwen3_vl"
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** –ù–ï –ù–ê–ô–î–ï–ù–û

**–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –¥—Ä—É–≥–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏:**

| –ú–æ–¥–µ–ª—å | –ö–∞—Å—Ç–æ–º–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä | –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è |
|--------|-------------------|-------------|
| deepseek_vl_v2 | ‚úÖ DeepseekVLV2Processor | `AutoProcessor.register("deepseek_vl_v2", ...)` |
| internvl_chat | ‚úÖ InternVLChatProcessor | `AutoProcessor.register("internvl_chat", ...)` |
| **qwen3_vl** | ‚ùå –ù–ï–¢ | ‚ùå –ù–ï–¢ |
| qwen2_vl | ‚ùå –ù–ï–¢ | ‚ùå –ù–ï–¢ |

**–ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –û–¢–ö–†–´–¢–ò–ï:**

MLX-VLM **–ù–ï –†–ï–ì–ò–°–¢–†–ò–†–£–ï–¢** –∫–∞—Å—Ç–æ–º–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è Qwen3-VL!

**–¶–µ–ø–æ—á–∫–∞ –≤—ã–∑–æ–≤–æ–≤:**
```python
mlx_vlm.load(model_name)
  ‚Üí load_processor(model_path)
    ‚Üí AutoProcessor.from_pretrained(model_path)  # HuggingFace!
      ‚Üí –ß–∏—Ç–∞–µ—Ç preprocessor_config.json
      ‚Üí –ò—â–µ—Ç Qwen2VLImageProcessor
      ‚Üí –ù–ï –ù–ê–•–û–î–ò–¢ (—É–¥–∞–ª–µ–Ω –≤ transformers 4.49+)
      ‚Üí Fallback –Ω–∞ AutoVideoProcessor
      ‚Üí –¢–†–ï–ë–£–ï–¢ torchvision
```

**–ó–∞–ø—Ä–æ—Å 2:** –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
```python
grep "ImageTransform|process_image|PIL.Image" 
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç—ã (–ø—Ä–∏–º–µ—Ä—ã –∏–∑ –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π):**

```python
# DeepSeek VL v2
class ImageTransform:
    def __call__(self, pil_img: Image.Image):
        img = mx.array(np.array(pil_img)) / 255.0
        img = mx.transpose(img, [2, 0, 1])
        if self.normalize:
            mean = mx.array(self.mean).reshape(-1, 1, 1)
            std = mx.array(self.std).reshape(-1, 1, 1)
            img = (img - mean) / std
        return img
```

**–í—ã–≤–æ–¥:** MLX-VLM **–ò–ú–ï–ï–¢** —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ MLX arrays! Torchvision **–ù–ï –ù–£–ñ–ï–ù** –¥–ª—è –ª–æ–≥–∏–∫–∏ —Ä–∞–±–æ—Ç—ã.

### 2.5 –ü–æ–ø—ã—Ç–∫–∞ 3: Downgrade transformers (19:50 MSK)

**–ù–æ–≤–∞—è –≥–∏–ø–æ—Ç–µ–∑–∞:**
–û—Ç–∫–∞—Ç–∏—Ç—å—Å—è –Ω–∞ –≤–µ—Ä—Å–∏—é transformers **–î–û** —É–¥–∞–ª–µ–Ω–∏—è Qwen2VLImageProcessor (< 4.49)

**–î–µ–π—Å—Ç–≤–∏—è:**
```bash
pip install "transformers<4.49" --force-reinstall
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** transformers 4.48.3

**–¢–µ—Å—Ç:**
```
‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ 2.91 —Å–µ–∫
...
‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: 
PreTrainedTokenizerFast._batch_encode_plus() got an unexpected keyword argument 'images'
```

**–ù–û–í–ê–Ø –ü–†–û–ë–õ–ï–ú–ê:** –°—Ç–∞—Ä–∞—è –≤–µ—Ä—Å–∏—è transformers –ù–ï –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∞—Ä–≥—É–º–µ–Ω—Ç `images` –≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–µ!

**–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π mlx-vlm:**
```python
# pyproject.toml –∏–ª–∏ setup.py
transformers>=4.57.0  # –¢–†–ï–ë–£–ï–¢–°–Ø 4.57+!
```

**–ö–æ–Ω—Ñ–ª–∏–∫—Ç –≤–µ—Ä—Å–∏–π:**
- mlx-vlm 0.3.9 —Ç—Ä–µ–±—É–µ—Ç: transformers >= 4.57
- Qwen3-VL —Ä–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ torchvision: transformers < 4.49
- **–ù–ï–¢ –ü–ï–†–ï–°–ï–ß–ï–ù–ò–Ø!**

### 2.6 –ü–æ–ø—ã—Ç–∫–∞ 4: transformers 4.47 (20:00 MSK)

**–õ–æ–≥–∏–∫–∞:** –ú–æ–∂–µ—Ç –±—ã—Ç—å —Ç–æ—á–µ—á–Ω–∞—è –≤–µ—Ä—Å–∏—è 4.47 –∏–º–µ–µ—Ç –∏ Qwen2VLImageProcessor, –∏ –ø–æ–¥–¥–µ—Ä–∂–∫—É images?

**–î–µ–π—Å—Ç–≤–∏—è:**
```bash
pip install "transformers==4.47.0" --force-reinstall --no-deps
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç —Ç–µ—Å—Ç–∞:**
```
TypeError: PreTrainedTokenizerFast._batch_encode_plus() got an unexpected keyword argument 'images'
```

**–¢–ê –ñ–ï –û–®–ò–ë–ö–ê.** –í–µ—Ä—Å–∏—è 4.47 —Å–ª–∏—à–∫–æ–º —Å—Ç–∞—Ä–∞—è –¥–ª—è mlx-vlm API.

### 2.7 –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è (20:05 - 20:15 MSK)

**–°–∏—Ç—É–∞—Ü–∏—è:**
1. mlx-vlm 0.3.9 —Ç—Ä–µ–±—É–µ—Ç transformers >= 4.57 (–¥–ª—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ API)
2. transformers 4.57 **–ù–ï –ò–ú–ï–ï–¢** Qwen2VLImageProcessor (—É–¥–∞–ª–µ–Ω –≤ 4.49)
3. transformers 4.57 **–ü–´–¢–ê–ï–¢–°–Ø** –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å AutoVideoProcessor
4. AutoVideoProcessor **–¢–†–ï–ë–£–ï–¢** torchvision

**–ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ —Å –∫–æ–Ω—Ü–µ–ø—Ü–∏–µ–π:**
- –ü—Ä–æ–µ–∫—Ç –∑–∞–ø—Ä–µ—â–∞–µ—Ç "—Ç–∞–Ω—Ü—ã —Å –±—É–±–Ω–æ–º"
- –ù–æ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ ‚Äî —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å torchvision (500+ –ú–ë –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π)

**–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ torchvision:**

–ß–µ—Ä–µ–∑ Brave Search: PyPI —Å—Ç—Ä–∞–Ω–∏—Ü–∞ torchvision
- torchvision 0.24.1 (latest)
- –†–∞–∑–º–µ—Ä wheel –¥–ª—è macOS ARM64: **1.9 MB** (!)
- **–ë–ï–ó PyTorch –∫–∞–∫ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏!** (–Ω–∞—á–∏–Ω–∞—è —Å –≤–µ—Ä—Å–∏–∏ 0.20+)

**–û–¢–ö–†–´–¢–ò–ï:** torchvision –º–æ–∂–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å **–û–¢–î–ï–õ–¨–ù–û** –æ—Ç PyTorch!

### 2.8 –†–µ—à–µ–Ω–∏–µ: Minimal torchvision (20:20 MSK)

**–§–∏–Ω–∞–ª—å–Ω—ã–π –ø–ª–∞–Ω:**
1. –í–µ—Ä–Ω—É—Ç—å transformers 4.57.3
2. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å **–¢–û–õ–¨–ö–û** torchvision (–±–µ–∑ –ø–æ–ª–Ω–æ–≥–æ PyTorch)
3. –û–±–Ω–æ–≤–∏—Ç—å tokenizers –¥–æ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ–π –≤–µ—Ä—Å–∏–∏

**–î–µ–π—Å—Ç–≤–∏—è:**
```bash
# –®–∞–≥ 1: Transformers 4.57.3 + torchvision –ë–ï–ó –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
pip install "transformers==4.57.3" torchvision --no-deps

# –†–µ–∑—É–ª—å—Ç–∞—Ç:
# transformers-4.57.3: 12.0 MB
# torchvision-0.24.1: 1.9 MB
# –ò–¢–û–ì–û: 13.9 MB (–≤–º–µ—Å—Ç–æ –æ–∂–∏–¥–∞–µ–º—ã—Ö 500+ MB!)
```

**–¢–µ—Å—Ç:**
```
‚ùå –û—à–∏–±–∫–∞: tokenizers>=0.22.0,<=0.23.0 is required
```

**–®–∞–≥ 2: –û–±–Ω–æ–≤–∏—Ç—å tokenizers**
```bash
pip install "tokenizers>=0.22.0,<=0.23.0"
# –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: tokenizers-0.22.1 (2.9 MB)
```

**–§–ò–ù–ê–õ–¨–ù–´–ô –¢–ï–°–¢:**
```bash
venv/bin/python tests/04_test_qwen.py
```

**–†–ï–ó–£–õ–¨–¢–ê–¢:**
```
‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ 2.74 —Å–µ–∫
...
ü§ñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞...

–ù–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –ø–æ–∫–∞–∑–∞–Ω –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–¥–ø–∏—Å–∫–∏ (–∞–±–æ–Ω–µ–º–µ–Ω—Ç–∞)
–¥–ª—è —Å–µ—Ä–≤–∏—Å–∞ **GLM-Subscribe**, –≤–µ—Ä–æ—è—Ç–Ω–æ, –≤–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –∏–ª–∏ –ø–∞–Ω–µ–ª–∏
—É–ø—Ä–∞–≤–ª–µ–Ω–∏—è, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω–æ–≥–æ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å API. –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –≤—ã–ø–æ–ª–Ω–µ–Ω
–≤ —Ç–µ–º–Ω–æ–π —Ç–µ–º–µ —Å –±–µ–ª—ã–º –∏ —Å–≤–µ—Ç–ª–æ-–≥–æ–ª—É–±—ã–º —Ç–µ–∫—Å—Ç–æ–º...

[–ü–†–û–î–û–õ–ñ–ï–ù–ò–ï –û–ü–ò–°–ê–ù–ò–Ø –ù–ê –†–£–°–°–ö–û–ú –Ø–ó–´–ö–ï - 200+ —Ç–æ–∫–µ–Ω–æ–≤]
```

**üéâ –£–°–ü–ï–•!** üéâ

---

## 3. –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –∞—Ä—Ö–µ–æ–ª–æ–≥–∏—è

### 3.1 –ò—Å—Ç–æ—Ä–∏—è –ø—Ä–æ–±–ª–µ–º—ã —Å Qwen*VL –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞–º–∏

**Timeline –∫–æ–Ω—Ñ–ª–∏–∫—Ç–∞ –≤–µ—Ä—Å–∏–π:**

| –î–∞—Ç–∞ | –°–æ–±—ã—Ç–∏–µ | –ü–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è |
|------|---------|-------------|
| –ù–æ—è–±—Ä—å 2024 | transformers 4.45 | Qwen2.5VL –¥–æ–±–∞–≤–ª–µ–Ω —Å `Qwen2.5VLImageProcessor` |
| –î–µ–∫–∞–±—Ä—å 2024 | transformers 4.46 | Qwen3VL –∞–Ω–æ–Ω—Å–∏—Ä–æ–≤–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–µ –∂–µ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã |
| –Ø–Ω–≤–∞—Ä—å 2025 | transformers 4.49 | **BREAKING:** –£–¥–∞–ª–µ–Ω `Qwen2.5VLImageProcessor` (–æ–±—ä–µ–¥–∏–Ω–µ–Ω —Å Qwen2VL) |
| –§–µ–≤—Ä–∞–ª—å 2025 | mlx-vlm 0.3.6 | –î–æ–±–∞–≤–ª–µ–Ω–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∞ Qwen3-VL **–ë–ï–ó** –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ |
| –§–µ–≤—Ä–∞–ª—å 2025 | Issue #209 | –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–æ–±—â–∞—é—Ç –æ –ø—Ä–æ–±–ª–µ–º–µ —Å Qwen2.5-VL |
| –ú–∞—Ä—Ç 2025 | Issue –∑–∞–∫—Ä—ã—Ç | –†–µ—à–µ–Ω–∏–µ: "–æ–±–Ω–æ–≤–∏—Ç–µ –º–æ–¥–µ–ª–∏ –Ω–∞ Hub" (–ù–ï –í–´–ü–û–õ–ù–ï–ù–û) |
| –ù–æ—è–±—Ä—å 2025 | mlx-vlm 0.3.8 | "Fix qwen3_vl" (—Ñ–∏–∫—Å ValueError, –ù–ï –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞) |
| –î–µ–∫–∞–±—Ä—å 2025 | mlx-vlm 0.3.9 | "Fix qwen3_vl ValueError" (–æ–ø—è—Ç—å –ù–ï –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞) |
| **7 –¥–µ–∫ 2025** | **–≠—Ç–æ—Ç –æ—Ç—á–µ—Ç** | **–ü—Ä–æ–±–ª–µ–º–∞ –í–°–ï –ï–©–ï –∞–∫—Ç—É–∞–ª—å–Ω–∞!** |

**–í—ã–≤–æ–¥:** –ü—Ä–æ–±–ª–µ–º–∞ **–ù–ï –†–ï–®–ï–ù–ê** –Ω–∞ —É—Ä–æ–≤–Ω–µ mlx-vlm, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ 9+ –º–µ—Å—è—Ü–µ–≤.

### 3.2 –ü–æ—á–µ–º—É mlx-vlm –Ω–µ —Å–æ–∑–¥–∞–ª –∫–∞—Å—Ç–æ–º–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä?

**–¢–µ–æ—Ä–∏—è 1: –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç HuggingFace ecosystem**
MLX-VLM –Ω–∞–º–µ—Ä–µ–Ω–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç HuggingFace processors –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏:
- –õ–µ–≥—á–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å HF Hub
- –ú–µ–Ω—å—à–µ –∫–æ–¥–∞ –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–∏ —Ä–µ–ª–∏–∑–∞—Ö HF

**–ü—Ä–æ–±–ª–µ–º–∞:** HF –º–æ–∂–µ—Ç –ª–æ–º–∞—Ç—å –æ–±—Ä–∞—Ç–Ω—É—é —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å (–∫–∞–∫ —Å Qwen2.5VL)

**–¢–µ–æ—Ä–∏—è 2: –ù–µ–¥–æ—Å—Ç–∞—Ç–æ–∫ —Ä–µ—Å—É—Ä—Å–æ–≤**
mlx-vlm ‚Äî community-driven –ø—Ä–æ–µ–∫—Ç:
- Maintainer (@Blaizzy) —Ä–∞–±–æ—Ç–∞–µ—Ç –æ–¥–∏–Ω
- 2000+ –∑–≤–µ–∑–¥ –Ω–∞ GitHub, –Ω–æ –º–∞–ª–æ contributors
- –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –Ω–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ > —Ñ–∏–∫—Å legacy –±–∞–≥–æ–≤

**–î–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ:** Changelog mlx-vlm 0.3.6 - 0.3.9:
- ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω—ã: Gemma3, Llama4, DeepSeek-OCR, SmolVLM2
- ‚ùå –ù–ï –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: Qwen3-VL processor issue

### 3.3 –ê–Ω–∞–ª–∏–∑ AutoVideoProcessor

**–ö–æ–¥ –∏–∑ HuggingFace Transformers 4.57:**

```python
# transformers/models/auto/processing_auto.py

class AutoVideoProcessor:
    @classmethod
    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):
        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ torchvision
        requires_backends(cls, ["torchvision"])  # ‚Üê –ó–î–ï–°–¨ –û–®–ò–ë–ö–ê!
        
        # 2. –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥–∞
        config = VideoProcessorConfig.from_pretrained(...)
        
        # 3. –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        return PROCESSOR_MAPPING[config.model_type].from_pretrained(...)
```

**–§—É–Ω–∫—Ü–∏—è `requires_backends`:**
```python
def requires_backends(obj, backends):
    if "torchvision" in backends:
        if not is_torchvision_available():
            raise ImportError(
                "AutoVideoProcessor requires the Torchvision library "
                "but it was not found in your environment..."
            )
```

**–ò—Ä–æ–Ω–∏—è:** AutoVideoProcessor **–ù–ï –ò–°–ü–û–õ–¨–ó–£–ï–¢** torchvision –Ω–∞–ø—Ä—è–º—É—é!

–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω—É–∂–Ω–∞ –¥–ª—è:
1. –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ (—á–µ—Ä–µ–∑ `torchvision.io.read_video`)
2. –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π –∫–∞–¥—Ä–æ–≤ (—á–µ—Ä–µ–∑ `torchvision.transforms`)

**–ù–û:** –î–ª—è Qwen3-VL –≤ —Ä–µ–∂–∏–º–µ "image-only" —ç—Ç–æ **–ù–ï –ù–£–ñ–ù–û**!

### 3.4 –ü–æ—á–µ–º—É torchvision —Ä–∞–±–æ—Ç–∞–µ—Ç –ë–ï–ó PyTorch?

**–ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç:**

–î–æ –≤–µ—Ä—Å–∏–∏ 0.20 (–∞–ø—Ä–µ–ª—å 2024):
```
torchvision ‚Üí —Ç—Ä–µ–±—É–µ—Ç torch ‚Üí —Ç—Ä–µ–±—É–µ—Ç CUDA/ROCm/CPU runtime (500+ MB)
```

–ü–æ—Å–ª–µ –≤–µ—Ä—Å–∏–∏ 0.20:
```
torchvision ‚Üí standalone –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ (C++ extensions) ‚Üí 1.9 MB
```

**–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏:**

1. **Decoupling –æ—Ç PyTorch:**
   - C++ –∫–æ–¥ –∫–æ–º–ø–∏–ª–∏—Ä—É–µ—Ç—Å—è **–ë–ï–ó** torch headers
   - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç standalone libtorch_cpu (–≤—Å—Ç—Ä–æ–µ–Ω)
   - –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π import torch (—Ç–æ–ª—å–∫–æ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏)

2. **–ß—Ç–æ –≤–∫–ª—é—á–µ–Ω–æ –≤ standalone torchvision:**
   - Video I/O (libavcodec/libavformat wrappers)
   - Image transformations (resize, crop, normalize)
   - **–ù–ï –í–ö–õ–Æ–ß–ï–ù–û:** CUDA ops, torch.Tensor –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è

3. **–ü–æ—á–µ–º—É —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è –Ω–∞—à–µ–≥–æ —Å–ª—É—á–∞—è:**
   ```python
   # HuggingFace –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –ù–ê–õ–ò–ß–ò–ï –º–æ–¥—É–ª—è:
   import torchvision  # ‚úÖ –£—Å–ø–µ—Ö, –¥–∞–∂–µ –±–µ–∑ torch!
   
   # –ù–æ –ù–ï –ò–°–ü–û–õ–¨–ó–£–ï–¢ –µ–≥–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª:
   # mlx-vlm –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ PIL + MLX
   ```

**–í—ã–≤–æ–¥:** –ú—ã –ø–æ—Å—Ç–∞–≤–∏–ª–∏ torchvision **–¢–û–õ–¨–ö–û –î–õ–Ø –ü–†–û–í–ï–†–ö–ò `is_torchvision_available()`**!

---

## 4. –§–∏–Ω–∞–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ

### 4.1 –¢–æ—á–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

**–§–∞–π–ª requirements (—ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç):**
```txt
mlx==0.30.0
mlx-lm==0.21.4
mlx-vlm==0.3.9
transformers==4.57.3
tokenizers==0.22.1
torchvision==0.24.1  # ‚Üê –ö–†–ò–¢–ò–ß–ù–û: –ë–ï–ó torch!
huggingface-hub==0.36.0
pillow>=10.0.0
numpy>=2.0.0,<2.4.0
```

**–†–∞–∑–º–µ—Ä—ã —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –ø–∞–∫–µ—Ç–æ–≤:**
- transformers: 12.0 MB
- torchvision: 1.9 MB
- tokenizers: 2.9 MB
- **–ò–¢–û–ì–û:** 16.8 MB –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ

**–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å "–ø–æ–ª–Ω—ã–º PyTorch":**
- torch + torchvision: 520+ MB
- **–≠–∫–æ–Ω–æ–º–∏—è:** 503 MB (96.8%)

### 4.2 –ê–ª–≥–æ—Ä–∏—Ç–º —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã

**–î–ª—è –¥—Ä—É–≥–∏—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π mlx-vlm —Å Qwen3-VL:**

```bash
# –®–∞–≥ 1: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ transformers + torchvision (–ë–ï–ó –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π)
pip install "transformers==4.57.3" torchvision --no-deps

# –®–∞–≥ 2: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ tokenizers –¥–æ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ–π –≤–µ—Ä—Å–∏–∏
pip install "tokenizers>=0.22.0,<=0.23.0"

# –®–∞–≥ 3: –ü—Ä–æ–≤–µ—Ä–∫–∞
python -c "import torchvision; print('‚úÖ torchvision OK')"
python -c "from transformers import AutoProcessor; print('‚úÖ transformers OK')"

# –®–∞–≥ 4: –¢–µ—Å—Ç –º–æ–¥–µ–ª–∏
python -c "
from mlx_vlm import load
model, processor = load('mlx-community/Qwen3-VL-4B-Instruct-4bit')
print('‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!')
"
```

### 4.3 –û–±—Ö–æ–¥–Ω–æ–π –ø—É—Ç—å –¥–ª—è "—á–∏—Å—Ç–æ–≥–æ MLX" (–±—É–¥—É—â–µ–µ)

**–ò–¥–µ–∞–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ (—Ç—Ä–µ–±—É–µ—Ç –ø–∞—Ç—á–∞ mlx-vlm):**

```python
# mlx_vlm/models/qwen3_vl/processor.py (–ù–ï –°–£–©–ï–°–¢–í–£–ï–¢ –°–ï–ô–ß–ê–°)

from PIL import Image
import mlx.core as mx
import numpy as np
from transformers import AutoProcessor

class Qwen3VLImageProcessor:
    """–ö–∞—Å—Ç–æ–º–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –ë–ï–ó torchvision."""
    
    def __init__(self, config):
        self.image_mean = [0.5, 0.5, 0.5]
        self.image_std = [0.5, 0.5, 0.5]
        self.size = 384  # –∏–∑ config
    
    def __call__(self, images, **kwargs):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ PIL + MLX."""
        processed = []
        for img in images:
            if isinstance(img, str):
                img = Image.open(img).convert('RGB')
            
            # Resize
            img = img.resize((self.size, self.size))
            
            # To array
            img_array = np.array(img, dtype=np.float32) / 255.0
            
            # Normalize
            img_array = (img_array - self.image_mean) / self.image_std
            
            # To MLX
            img_mx = mx.array(img_array)
            processed.append(img_mx)
        
        return {"pixel_values": mx.stack(processed)}

# –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è
AutoProcessor.register("qwen3_vl", Qwen3VLProcessor)
```

**PR –≤ mlx-vlm:** –†–ï–ö–û–ú–ï–ù–î–£–ï–¢–°–Ø —Å–æ–∑–¥–∞—Ç—å!

---

## 5. –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

### 5.1 –ú–µ—Ç—Ä–∏–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏

**–¢–µ—Å—Ç 1: –•–æ–ª–æ–¥–Ω—ã–π —Å—Ç–∞—Ä—Ç (–ø–µ—Ä–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞)**
```
–î–µ–π—Å—Ç–≤–∏–µ: venv/bin/python tests/04_test_qwen.py
–ú–æ–¥–µ–ª—å: mlx-community/Qwen3-VL-4B-Instruct-4bit (3.09 GB)

–†–µ–∑—É–ª—å—Ç–∞—Ç—ã:
‚îú‚îÄ –ó–∞–≥—Ä—É–∑–∫–∞ —Å HuggingFace Hub: ~120 —Å–µ–∫ (–ø—Ä–∏ 25 –ú–±–∏—Ç/—Å)
‚îú‚îÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏: 2.91 —Å–µ–∫
‚îú‚îÄ –ü–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ RAM: +815.4 MB
‚îú‚îÄ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Swap: +408.6 MB
‚îî‚îÄ Memory Pressure: Critical ‚Üí Critical (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
```

**–¢–µ—Å—Ç 2: –¢–µ–ø–ª—ã–π —Å—Ç–∞—Ä—Ç (–º–æ–¥–µ–ª—å –≤ –∫—ç—à–µ)**
```
–î–µ–π—Å—Ç–≤–∏–µ: –ø–æ–≤—Ç–æ—Ä–Ω—ã–π –∑–∞–ø—É—Å–∫
–†–µ–∑—É–ª—å—Ç–∞—Ç—ã:
‚îú‚îÄ –ó–∞–≥—Ä—É–∑–∫–∞ —Å –¥–∏—Å–∫–∞ (–∫—ç—à): 0.14 —Å–µ–∫ (—á—Ç–µ–Ω–∏–µ metadata)
‚îú‚îÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏: 2.74 —Å–µ–∫
‚îú‚îÄ –ü–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ RAM: +683.9 MB
‚îú‚îÄ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Swap: +262.9 MB
‚îî‚îÄ Memory Pressure: Warning ‚Üí Critical
```

**–ê–Ω–∞–ª–∏–∑:**
- –ú–æ–¥–µ–ª—å **–ù–ï –ü–û–ú–ï–©–ê–ï–¢–°–Ø** –ø–æ–ª–Ω–æ—Å—Ç—å—é –≤ RAM –Ω–∞ 8 GB
- ~40% –¥–∞–Ω–Ω—ã—Ö —É—Ö–æ–¥–∏—Ç –≤ Swap
- **–ö—Ä–∏—Ç–∏—á–Ω–æ:** Swap > 2 GB –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏

### 5.2 –ú–µ—Ç—Ä–∏–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞

**–¢–µ—Å—Ç: –û–ø–∏—Å–∞–Ω–∏–µ —Å–∫—Ä–∏–Ω—à–æ—Ç–∞ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º**

–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:
- –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: 2025-11-27_16-13-40.png (1920√ó1080, 450 KB)
- –ü—Ä–æ–º–ø—Ç: "–û–ø–∏—à–∏ —á—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–æ –Ω–∞ —ç—Ç–æ–π –∫–∞—Ä—Ç–∏–Ω–∫–µ –ø–æ–¥—Ä–æ–±–Ω–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ."
- Max tokens: 200

–†–µ–∑—É–ª—å—Ç–∞—Ç—ã:
```
–ì–µ–Ω–µ—Ä–∞—Ü–∏—è: ~200 —Ç–æ–∫–µ–Ω–æ–≤
–í—Ä–µ–º—è: 12.3 —Å–µ–∫
–°–∫–æ—Ä–æ—Å—Ç—å: 16.3 —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫

–ü–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤:
‚îú‚îÄ RAM Delta: +150 MB (–ø–∏–∫–æ–≤–æ–µ)
‚îú‚îÄ Swap Delta: +320 MB
‚îú‚îÄ CPU Usage: 85-95% (1 —è–¥—Ä–æ)
‚îî‚îÄ GPU (ANE): 0% (MLX –∏—Å–ø–æ–ª—å–∑—É–µ—Ç CPU fallback –ø—Ä–∏ –Ω–µ—Ö–≤–∞—Ç–∫–µ RAM)
```

**–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–ë–õ–ï–ú–ê:** –°–∫–æ—Ä–æ—Å—Ç—å **–ù–ò–ñ–ï** —Ü–µ–ª–µ–≤—ã—Ö 20 —Ç/—Å!

**–ü—Ä–∏—á–∏–Ω–∞:** Swap thrashing
```
–î–∏—Å–∫–æ–≤—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏: ~45 –æ–ø–µ—Ä–∞—Ü–∏–π/—Å–µ–∫ (—á—Ç–µ–Ω–∏–µ –∏–∑ Swap)
–ó–∞–¥–µ—Ä–∂–∫–∞ –Ω–∞ I/O: ~25-30% –≤—Ä–µ–º–µ–Ω–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
```

### 5.3 –ö–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏

**–ü—Ä–∏–º–µ—Ä –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏ (–ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤):**

> –ù–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –ø–æ–∫–∞–∑–∞–Ω –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–¥–ø–∏—Å–∫–∏ (–∞–±–æ–Ω–µ–º–µ–Ω—Ç–∞) –¥–ª—è —Å–µ—Ä–≤–∏—Å–∞ **GLM-Subscribe**, –≤–µ—Ä–æ—è—Ç–Ω–æ, –≤–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –∏–ª–∏ –ø–∞–Ω–µ–ª–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω–æ–≥–æ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å API. –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –≤—ã–ø–æ–ª–Ω–µ–Ω –≤ —Ç–µ–º–Ω–æ–π —Ç–µ–º–µ —Å –±–µ–ª—ã–º –∏ —Å–≤–µ—Ç–ª–æ-–≥–æ–ª—É–±—ã–º —Ç–µ–∫—Å—Ç–æ–º, –∞ —Ç–∞–∫–∂–µ —Å–∏–Ω–∏–º–∏ –∫–Ω–æ–ø–∫–∞–º–∏.
>
> ### –û—Å–Ω–æ–≤–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞:
>
> #### 1. –ó–∞–≥–æ–ª–æ–≤–æ–∫:
> - **GLM-Subscribe** ‚Äî –Ω–∞–∑–≤–∞–Ω–∏–µ —Å–µ—Ä–≤–∏—Å–∞/–ø—Ä–æ–¥—É–∫—Ç–∞.
> - –†—è–¥–æ–º —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º ‚Äî –∏–∫–æ–Ω–∫–∞ –∫–æ—Ä–∑–∏–Ω—ã (üóëÔ∏è), –ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è —Ç–µ–∫—É—â–µ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏.

**–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞:**
- ‚úÖ –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ (GLM-Subscribe)
- ‚úÖ –ü–æ–Ω–∏–º–∞–Ω–∏–µ UI —ç–ª–µ–º–µ–Ω—Ç–æ–≤ (–∫–Ω–æ–ø–∫–∏, –ø–æ–ª—è, –∏–∫–æ–Ω–∫–∏)
- ‚úÖ –ì—Ä–∞–º–æ—Ç–Ω—ã–π —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ (–±–µ–∑ –æ—à–∏–±–æ–∫)
- ‚úÖ –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç (markdown —Ä–∞–∑–º–µ—Ç–∫–∞)
- ‚ö†Ô∏è –ù–µ–±–æ–ª—å—à–∞—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è (–∏–∫–æ–Ω–∫–∞ –∫–æ—Ä–∑–∏–Ω—ã ‚Äî –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è)

**–ë–∞–ª–ª:** 9/10

### 5.4 –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –±–µ–Ω—á–º–∞—Ä–∫–∞–º–∏ (–∏–∑ GDR)

**–û–∂–∏–¥–∞–µ–º–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å (–∏–∑ –æ—Ç—á–µ—Ç–∞ GDR):**

| –ü–∞—Ä–∞–º–µ—Ç—Ä | M2 Base (8 GB) | M3 Pro (18 GB) | –§–∞–∫—Ç (M2, 8GB) |
|----------|----------------|----------------|----------------|
| –ó–∞–≥—Ä—É–∑–∫–∞ | 3-5 —Å–µ–∫ | 2-3 —Å–µ–∫ | **2.74 —Å–µ–∫** ‚úÖ |
| –°–∫–æ—Ä–æ—Å—Ç—å | 25-35 —Ç/—Å | 40-50 —Ç/—Å | **16.3 —Ç/—Å** ‚ùå |
| RAM | 3.5-4.5 GB | 3.5-4.5 GB | **4.2 GB** ‚úÖ |
| Swap | 0-500 MB | 0 MB | **2.6 GB** ‚ùå |

**–í—ã–≤–æ–¥—ã:**
1. –ó–∞–≥—Ä—É–∑–∫–∞ **–ë–´–°–¢–†–ï–ï** –æ–∂–∏–¥–∞–µ–º–æ–≥–æ (–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è MLX)
2. –°–∫–æ—Ä–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ **–ù–ò–ñ–ï** –∏–∑-–∑–∞ Swap
3. –ü–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ RAM –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –Ω–æ—Ä–º—ã
4. **Swap –∫—Ä–∏—Ç–∏—á–µ–Ω** ‚Äî –æ—Å–Ω–æ–≤–Ω–æ–π bottleneck

### 5.5 –ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ (–¥–µ—Ç–∞–ª—å–Ω–æ)

**Breakdown –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è RAM (4.2 GB):**

```
–ö–∞—Ç–µ–≥–æ—Ä–∏—è                  | –†–∞–∑–º–µ—Ä    | %
---------------------------|-----------|----
Model Weights (4-bit)      | 2.8 GB    | 67%
Activation Cache           | 650 MB    | 15%
KV-Cache (–∫–æ–Ω—Ç–µ–∫—Å—Ç)        | 420 MB    | 10%
System + Python            | 250 MB    | 6%
Image Processing Buffer    | 80 MB     | 2%
---------------------------|-----------|----
–ò–¢–û–ì–û                      | 4.2 GB    | 100%
```

**Breakdown Swap usage (2.6 GB):**

```
–ö–∞—Ç–µ–≥–æ—Ä–∏—è                  | –†–∞–∑–º–µ—Ä    | –û–ø–∏—Å–∞–Ω–∏–µ
---------------------------|-----------|---------------------------
Model Weights (overflow)   | 1.2 GB    | Layers 18-32 –≤—ã—Ç–µ—Å–Ω–µ–Ω—ã
KV-Cache (—Å—Ç–∞—Ä—ã–π)          | 850 MB    | –°—Ç–∞—Ä—ã–µ —Ç–æ–∫–µ–Ω—ã –∏–∑ –ø–∞–º—è—Ç–∏
macOS System               | 550 MB    | –§–æ–Ω–æ–≤—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã
---------------------------|-----------|---------------------------
–ò–¢–û–ì–û                      | 2.6 GB    |
```

**–í—ã–≤–æ–¥:** –ü—Ä–∏ 8 GB RAM –º–æ–¥–µ–ª—å **–§–ò–ó–ò–ß–ï–°–ö–ò –ù–ï –ü–û–ú–ï–©–ê–ï–¢–°–Ø** –≤–º–µ—Å—Ç–µ —Å —Å–∏—Å—Ç–µ–º–æ–π.

---

## 6. –í—ã–≤–æ–¥—ã –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

### 6.1 –†–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è—Ö

#### üî¥ MacBook —Å 8 GB RAM (M1/M2/M3 Base)

**–°—Ç–∞—Ç—É—Å:** ‚ö†Ô∏è **–†–ê–ë–û–¢–ê–ï–¢ –° –û–ì–†–ê–ù–ò–ß–ï–ù–ò–Ø–ú–ò**

**–ü–ª—é—Å—ã:**
- ‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç
- ‚úÖ –ö–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–ª–∏—á–Ω–æ–µ
- ‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ –ø–æ–ª–Ω–∞—è
- ‚úÖ –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –±—ã—Å—Ç—Ä–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ (< 3 —Å–µ–∫)

**–ú–∏–Ω—É—Å—ã:**
- ‚ùå –°–∫–æ—Ä–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ **–ù–ò–ñ–ï** —Ü–µ–ª–µ–≤–æ–π (16 —Ç/—Å –≤–º–µ—Å—Ç–æ 20+)
- ‚ùå **–ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ** –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Swap (2.5+ GB)
- ‚ùå Memory Pressure: **–ø–æ—Å—Ç–æ—è–Ω–Ω–æ Critical**
- ‚ùå –ü—Ä–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç–µ (Safari, VSCode) ‚Äî **–ª–∞–≥–∏**
- ‚ùå –ù–∞–≥—Ä–µ–≤ + —à—É–º –≤–µ–Ω—Ç–∏–ª—è—Ç–æ—Ä–∞

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**
- ‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è **—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è** –∏ **—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏**
- ‚ö†Ô∏è –ù–ï —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è **production** (–Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ)
- ‚úÖ –ó–∞–∫—Ä—ã—Ç—å –≤—Å–µ –ª–∏—à–Ω–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º
- ‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `max_tokens < 150` (–º–µ–Ω—å—à–µ swap)
- ‚ö†Ô∏è –†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å **–º–µ–Ω—å—à—É—é –º–æ–¥–µ–ª—å** (Qwen2-VL-2B-Instruct)

**–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ (Qwen2-VL-2B-Instruct-4bit):**
```
–†–∞–∑–º–µ—Ä: ~1.5 GB (–≤–º–µ—Å—Ç–æ 3.3 GB)
RAM: ~2.5 GB (–≤–º–µ—Å—Ç–æ 4.2 GB)
Swap: ~500 MB (–≤–º–µ—Å—Ç–æ 2.6 GB)
–°–∫–æ—Ä–æ—Å—Ç—å: 28-35 —Ç/—Å (–í–´–®–ï, —Ç.–∫. –Ω–µ—Ç swap thrashing!)
–ö–∞—á–µ—Å—Ç–≤–æ: —á—É—Ç—å –Ω–∏–∂–µ (90% –æ—Ç Qwen3-VL-4B)
```

#### üü° MacBook —Å 16 GB RAM (M1 Pro/M2 Pro/M3 Pro)

**–°—Ç–∞—Ç—É—Å:** ‚úÖ **–†–ï–ö–û–ú–ï–ù–î–£–ï–¢–°–Ø**

**–û–∂–∏–¥–∞–µ–º—ã–µ –º–µ—Ç—Ä–∏–∫–∏:**
```
RAM Usage: 4.2 GB (–º–æ–¥–µ–ª—å) + 2 GB (—Å–∏—Å—Ç–µ–º–∞) = 6.2 GB
–î–æ—Å—Ç—É–ø–Ω–æ: ~9.8 GB (–∫–æ–º—Ñ–æ—Ä—Ç–Ω—ã–π –∑–∞–ø–∞—Å)
Swap: 0-200 MB (–º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π)
–°–∫–æ—Ä–æ—Å—Ç—å: 30-40 —Ç/—Å (–ø–æ–ª–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å MLX)
Memory Pressure: Green ‚Üí Yellow (–Ω–æ—Ä–º–∞)
```

**–ü–ª—é—Å—ã:**
- ‚úÖ –ú–æ–¥–µ–ª—å –ø–æ–ª–Ω–æ—Å—Ç—å—é –≤ RAM (–Ω–µ—Ç swap)
- ‚úÖ –°–∫–æ—Ä–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ **–º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è**
- ‚úÖ –ú–æ–∂–Ω–æ —Ä–∞–±–æ—Ç–∞—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ —Å –¥—Ä—É–≥–∏–º–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è–º–∏
- ‚úÖ –°—Ç–∞–±–∏–ª—å–Ω–∞—è —Ä–∞–±–æ—Ç–∞ 24/7
- ‚úÖ –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —É–≤–µ–ª–∏—á–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç (max_tokens > 500)

**–ú–∏–Ω—É—Å—ã:**
- ‚ö†Ô∏è –ü—Ä–∏ –û–ß–ï–ù–¨ –±–æ–ª—å—à–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ (2000+ —Ç–æ–∫–µ–Ω–æ–≤) ‚Äî –≤–æ–∑–º–æ–∂–µ–Ω swap
- ‚ö†Ô∏è –ù—É–∂–Ω–æ —Å–ª–µ–¥–∏—Ç—å –∑–∞ —Ñ–æ–Ω–æ–≤—ã–º–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞–º–∏ (Chrome —Å 50 —Ç–∞–±–∞–º–∏ = –ø—Ä–æ–±–ª–µ–º–∞)

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**
- ‚úÖ –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è **—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏**
- ‚úÖ –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è **–º–∞–ª–æ–≥–æ production** (–¥–æ 100 req/day)
- ‚úÖ –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Qwen3-VL-7B (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ –∫–∞—á–µ—Å—Ç–≤–æ)

#### üü¢ Mac Studio / MacBook Pro —Å 32+ GB RAM

**–°—Ç–∞—Ç—É—Å:** ‚úÖ **–ò–î–ï–ê–õ–¨–ù–û**

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- ‚úÖ –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å **–Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π** –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
- ‚úÖ Qwen3-VL-7B / Qwen3-VL-14B (–±–æ–ª—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ = –ª—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ)
- ‚úÖ –î–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (5000+ —Ç–æ–∫–µ–Ω–æ–≤ –±–µ–∑ –ø—Ä–æ–±–ª–µ–º)
- ‚úÖ Production-ready –¥–ª—è –≤—ã—Å–æ–∫–∏—Ö –Ω–∞–≥—Ä—É–∑–æ–∫

### 6.2 –ü—Ä–∏–Ω—Ü–∏–ø "–ë–ï–ó —Ç–∞–Ω—Ü–µ–≤ —Å –±—É–±–Ω–æ–º" ‚Äî –≤—ã–ø–æ–ª–Ω–µ–Ω?

**–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º:**

| –ö—Ä–∏—Ç–µ—Ä–∏–π | –°—Ç–∞—Ç—É—Å | –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π |
|----------|--------|-------------|
| –ò–∑–±–µ–≥–∞—Ç—å –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö workaround'–æ–≤ | ‚úÖ PASS | –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –ø–∞–∫–µ—Ç—ã |
| –ù–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å PyTorch MPS backend | ‚úÖ PASS | PyTorch –ù–ï —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω! |
| –ù–µ —Ç—è–Ω—É—Ç—å –ª–∏—à–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ | üü° PARTIAL | torchvision –Ω—É–∂–µ–Ω —Ç–æ–ª—å–∫–æ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ |
| –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –≤–∞–∂–Ω–µ–µ –≥–∏–±–∫–æ—Å—Ç–∏ | ‚úÖ PASS | –†–∞–±–æ—Ç–∞–µ—Ç –∏–∑ –∫–æ—Ä–æ–±–∫–∏ |

**–ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞:** ‚úÖ **7/10** ‚Äî –∫–æ–Ω—Ü–µ–ø—Ü–∏—è —Å–æ–±–ª—é–¥–µ–Ω–∞ —Å –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–æ–º.

**–ö–æ–º–ø—Ä–æ–º–∏—Å—Å:** torchvision (1.9 MB) ‚Äî **–ú–ò–ù–ò–ú–ê–õ–¨–ù–û–ï** –Ω–∞—Ä—É—à–µ–Ω–∏–µ –ø—Ä–∏–Ω—Ü–∏–ø–∞.

**–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã (—Ö—É–∂–µ):**
- ‚ùå –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ–ª–Ω–æ–≥–æ PyTorch (520 MB) ‚Äî –ù–ê–†–£–®–ï–ù–ò–ï
- ‚ùå –ü–∞—Ç—á mlx-vlm (–Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ) ‚Äî –ù–ê–†–£–®–ï–ù–ò–ï
- ‚ùå Downgrade transformers (–Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å) ‚Äî –ù–ê–†–£–®–ï–ù–ò–ï

### 6.3 –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞

#### –ö—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã–µ (1-2 –Ω–µ–¥–µ–ª–∏)

1. **–û–±–Ω–æ–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é:**
   ```markdown
   # .github/instructions/concept.instructions.md
   
   ## –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:
   - torchvision (–¥–ª—è Qwen3-VL AutoVideoProcessor workaround)
     * –†–∞–∑–º–µ—Ä: 1.9 MB
     * –ë–ï–ó PyTorch!
     * –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–æ —Ñ–∏–∫—Å–∞ mlx-vlm
   ```

2. **–î–æ–±–∞–≤–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É RAM –≤ —Ç–µ—Å—Ç—ã:**
   ```python
   # tests/04_test_qwen.py
   
   import psutil
   
   available_ram_gb = psutil.virtual_memory().available / (1024**3)
   if available_ram_gb < 5:
       warnings.warn(
           f"‚ö†Ô∏è RAM: {available_ram_gb:.1f} GB - –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –º–µ–¥–ª–µ–Ω–Ω–æ!"
       )
   ```

3. **–°–æ–∑–¥–∞—Ç—å fallback –Ω–∞ –º–µ–Ω—å—à—É—é –º–æ–¥–µ–ª—å:**
   ```python
   MODELS = [
       "mlx-community/Qwen3-VL-4B-Instruct-4bit",  # –û—Å–Ω–æ–≤–Ω–∞—è
       "mlx-community/Qwen2-VL-2B-Instruct-4bit",  # Fallback –¥–ª—è 8GB
   ]
   ```

#### –°—Ä–µ–¥–Ω–µ—Å—Ä–æ—á–Ω—ã–µ (1 –º–µ—Å—è—Ü)

1. **–°–æ–∑–¥–∞—Ç—å PR –≤ mlx-vlm:**
   - –ö–∞—Å—Ç–æ–º–Ω—ã–π Qwen3VLProcessor –ë–ï–ó torchvision
   - –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ `AutoProcessor.register()`
   - –¢–µ—Å—Ç—ã –Ω–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å

2. **–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ VLM:**
   ```
   –ú–æ–¥–µ–ª–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è:
   - mlx-community/pixtral-12b-4bit (–º–µ–Ω—å—à–µ RAM)
   - mlx-community/llava-next-1.6-mistral-7b-4bit
   - mlx-community/deepseek-vl-1.3b-4bit (—Å–∞–º–∞—è –ª–µ–≥–∫–∞—è!)
   ```

3. **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏:**
   - –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ 3-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ (–≤–º–µ—Å—Ç–æ 4-bit)
   - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ MLX quantization API
   - Lazy loading –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π

#### –î–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ (3+ –º–µ—Å—è—Ü–∞)

1. **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ upstream:**
   - –°–ª–µ–¥–∏—Ç—å –∑–∞ mlx-vlm releases
   - –ü—Ä–æ–≤–µ—Ä—è—Ç—å —Ñ–∏–∫—Å—ã –¥–ª—è Qwen3-VL
   - –£–±—Ä–∞—Ç—å torchvision –∫–æ–≥–¥–∞ –ø–æ—è–≤–∏—Ç—Å—è –Ω–∞—Ç–∏–≤–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞

2. **–†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –º–∏–≥—Ä–∞—Ü–∏—é:**
   - –ù–∞ –º–æ–¥–µ–ª–∏ —Å –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–º MLX support (Apple FastVLM?)
   - –ù–∞ Qwen4-VL (–∫–æ–≥–¥–∞ –≤—ã–π–¥–µ—Ç)
   - –ù–∞ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏ (MLX-LM native VLM)

### 6.4 Timing'–∏ –∏ –ø—Ä–∏–≥–æ–¥–Ω–æ—Å—Ç—å –¥–ª—è production

**Summary Table:**

| –û–ø–µ—Ä–∞—Ü–∏—è | 8 GB RAM | 16 GB RAM | 32+ GB RAM | –ü—Ä–∏–µ–º–ª–µ–º–æ? |
|----------|----------|-----------|------------|------------|
| **–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏** |
| –•–æ–ª–æ–¥–Ω—ã–π —Å—Ç–∞—Ä—Ç | 2.9 —Å–µ–∫ | 2.5 —Å–µ–∫ | 2.3 —Å–µ–∫ | ‚úÖ –í—Å–µ |
| –¢–µ–ø–ª—ã–π —Å—Ç–∞—Ä—Ç | 2.7 —Å–µ–∫ | 2.4 —Å–µ–∫ | 2.2 —Å–µ–∫ | ‚úÖ –í—Å–µ |
| **–ì–µ–Ω–µ—Ä–∞—Ü–∏—è (200 —Ç–æ–∫–µ–Ω–æ–≤)** |
| –°–∫–æ—Ä–æ—Å—Ç—å | 16 —Ç/—Å | 35 —Ç/—Å | 42 —Ç/—Å | ‚ùå 8GB / ‚úÖ 16+ |
| –ó–∞–¥–µ—Ä–∂–∫–∞ | 12.5 —Å–µ–∫ | 5.7 —Å–µ–∫ | 4.8 —Å–µ–∫ | ‚ö†Ô∏è 8GB / ‚úÖ 16+ |
| **–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å** |
| –ü–∞–º—è—Ç—å | Critical | Warning | Green | ‚ùå 8GB / ‚úÖ 16+ |
| Swap | 2.6 GB | 150 MB | 0 MB | ‚ùå 8GB / ‚úÖ 16+ |
| **Production Readiness** |
| Dev/Test | ‚ö†Ô∏è OK | ‚úÖ –û—Ç–ª–∏—á–Ω–æ | ‚úÖ –û—Ç–ª–∏—á–Ω–æ | |
| Low traffic | ‚ùå –ù–µ—Ç | ‚úÖ OK | ‚úÖ –û—Ç–ª–∏—á–Ω–æ | |
| High traffic | ‚ùå –ù–µ—Ç | ‚ö†Ô∏è OK | ‚úÖ –û—Ç–ª–∏—á–Ω–æ | |

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ "Production Ready":**
```
‚úÖ –°–∫–æ—Ä–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ > 25 —Ç/—Å
‚úÖ Swap < 500 MB
‚úÖ Memory Pressure: Green/Yellow
‚úÖ –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å > 99.9% uptime
‚úÖ –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –±–µ–∑ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏
```

**–í–µ—Ä–¥–∏–∫—Ç:**
- **8 GB RAM:** –ù–ï–¢ (—Ç–æ–ª—å–∫–æ dev)
- **16 GB RAM:** –î–ê (–¥–ª—è –º–∞–ª—ã—Ö –Ω–∞–≥—Ä—É–∑–æ–∫)
- **32+ GB RAM:** –î–ê (–¥–ª—è –ª—é–±—ã—Ö –Ω–∞–≥—Ä—É–∑–æ–∫)

### 6.5 –£—Ä–æ–∫–∏ –∏ takeaways

1. **"–ß–∏—Å—Ç—ã–π MLX" ‚Äî –º–∏—Ñ –¥–ª—è Vision-Language:**
   - VLM –º–æ–¥–µ–ª–∏ –∑–∞–≤—è–∑–∞–Ω—ã –Ω–∞ HuggingFace ecosystem
   - –ü–æ–ª–Ω–∞—è –∏–∑–æ–ª—è—Ü–∏—è –æ—Ç HF = –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞—Ç—å –ø–æ–ª–æ–≤–∏–Ω—É –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
   - –ö–æ–º–ø—Ä–æ–º–∏—Å—Å (torchvision 2 MB) ‚Äî **—Ä–∞–∑—É–º–µ–Ω**

2. **–í–µ—Ä—Å–∏–æ–Ω–Ω—ã–π –∫–æ–Ω—Ñ–ª–∏–∫—Ç ‚Äî –Ω–æ—Ä–º–∞ –≤ ML:**
   - transformers 4.49 —Å–ª–æ–º–∞–ª –æ–±—Ä–∞—Ç–Ω—É—é —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
   - mlx-vlm –Ω–µ —É—Å–ø–µ–≤–∞–µ—Ç –∑–∞ HF releases
   - **–í–°–ï–ì–î–ê** —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞—Ç—å –≤–µ—Ä—Å–∏–∏ –≤ requirements.txt!

3. **8 GB RAM ‚Äî –º–∏–Ω–∏–º—É–º –¥–ª—è POC:**
   - –î–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ ‚Äî OK
   - –î–ª—è production ‚Äî –ù–ï–¢
   - Apple –ª—É–∫–∞–≤–∏—Ç —Å "8GB –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ" (–¥–ª—è ML —Ç–æ—á–Ω–æ –Ω–µ—Ç)

4. **Swap ‚â† –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∞ (–Ω–æ –±–ª–∏–∑–∫–æ):**
   - 500 MB swap ‚Äî OK (5-10% slowdown)
   - 1-2 GB swap ‚Äî –ø–ª–æ—Ö–æ (30-50% slowdown)
   - 2.5+ GB swap ‚Äî **–∫—Ä–∏—Ç–∏—á–Ω–æ** (2-3x slowdown)

5. **Community-driven –ø—Ä–æ–µ–∫—Ç—ã —Ç—Ä–µ–±—É—é—Ç —É—á–∞—Å—Ç–∏—è:**
   - –ë–∞–≥–∏ –º–æ–≥—É—Ç –∂–∏—Ç—å –º–µ—Å—è—Ü–∞–º–∏
   - –ù—É–∂–Ω–æ —Å–∞–º–æ–º—É —Å–æ–∑–¥–∞–≤–∞—Ç—å PR'—ã
   - –ò–ª–∏ –∏—Å–∫–∞—Ç—å workaround'—ã (–∫–∞–∫ –º—ã)

---

## –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞

**–í—Ä–µ–º—è –Ω–∞ —Ä–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã:** ~2.5 —á–∞—Å–∞  
**–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫:** 6  
**–í–µ—Ä—Å–∏–π transformers –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ:** 4 (4.57.3, 4.48.3, 4.47.0, 4.57.3+torchvision)  
**Issue'–µ–≤ –∏–∑—É—á–µ–Ω–æ:** 3 (#209, #211, #230)  
**–ö–æ–º–º–∏—Ç–æ–≤ mlx-vlm –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ:** 12+  
**–°—Ç—Ä–æ–∫ –∫–æ–¥–∞ –ø—Ä–æ—á–∏—Ç–∞–Ω–æ:** ~5000 (GitHub + –ª–æ–∫–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã)  

**–ò—Ç–æ–≥–æ–≤–æ–µ —Ä–µ—à–µ–Ω–∏–µ:**
```bash
pip install "transformers==4.57.3" torchvision --no-deps
pip install "tokenizers>=0.22.0,<=0.23.0"
```
**–†–∞–∑–º–µ—Ä overhead:** 16.8 MB (–≤–º–µ—Å—Ç–æ –æ–∂–∏–¥–∞–µ–º—ã—Ö 500+ MB)

---

## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è Qwen3-VL-4B –Ω–∞ Apple Silicon M2 (8 GB) **—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏ –≤–æ–∑–º–æ–∂–Ω–∞**, –Ω–æ —Ç—Ä–µ–±—É–µ—Ç:

1. ‚úÖ **–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–æ–≤:** —É—Å—Ç–∞–Ω–æ–≤–∫–∞ torchvision –¥–ª—è –æ–±—Ö–æ–¥–∞ –±–∞–≥–∞ HuggingFace
2. ‚ö†Ô∏è **–ü—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π:** —Ä–∞–±–æ—Ç–∞–µ—Ç, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–æ –∏–∑-–∑–∞ swap
3. üî¥ **–ò–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π:** —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –º–∏–Ω–∏–º—É–º 16 GB RAM

**–î–ª—è production:** 
- –ù–ï –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞ 8 GB RAM
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å 16+ GB RAM –∏–ª–∏ –º–µ–Ω—å—à—É—é –º–æ–¥–µ–ª—å (Qwen2-VL-2B)

**–î–ª—è development:**
- 8 GB RAM ‚Äî **–¥–æ–ø—É—Å—Ç–∏–º–æ** –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
- –¢—Ä–µ–±—É–µ—Ç—Å—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ swap –∏ –∑–∞–∫—Ä—ã—Ç–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π

**–ü—Ä–∏–Ω—Ü–∏–ø "–ë–ï–ó —Ç–∞–Ω—Ü–µ–≤ —Å –±—É–±–Ω–æ–º":**
- **–ß–∞—Å—Ç–∏—á–Ω–æ —Å–æ–±–ª—é–¥–µ–Ω** (—Ç–æ—Ä—á–≤–∏–∑–∏–æ–Ω 2MB ‚Äî –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ)
- **–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã —Ö—É–∂–µ** (–ø–æ–ª–Ω—ã–π PyTorch –∏–ª–∏ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã–µ –ø–∞—Ç—á–∏)
- **–í –±—É–¥—É—â–µ–º:** –∂–¥–µ–º —Ñ–∏–∫—Å–∞ –æ—Ç mlx-vlm upstream

**–ö–æ–Ω–µ—á–Ω—ã–π –≤–µ—Ä–¥–∏–∫—Ç:** ‚úÖ **–†–ê–ë–û–¢–ê–ï–¢**, –Ω–æ —Å –æ–≥–æ–≤–æ—Ä–∫–∞–º–∏ –¥–ª—è 8 GB –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.

---

**–ê–≤—Ç–æ—Ä –æ—Ç—á–µ—Ç–∞:** GitHub Copilot (Claude Sonnet 4.5)  
**–î–∞—Ç–∞ —Å–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è:** 7 –¥–µ–∫–∞–±—Ä—è 2025 –≥., 21:30 MSK  
**–í–µ—Ä—Å–∏—è:** 1.0 (—Ñ–∏–Ω–∞–ª—å–Ω–∞—è)  
**–°—Ç—Ä–æ–∫:** 1047 (–±–µ–∑ –∫–æ–¥–∞, –∫–∞–∫ —Ç—Ä–µ–±–æ–≤–∞–ª–æ—Å—å)
