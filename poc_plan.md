Володя, принято. План составлен жестко, по принципу **"Fail Fast"** (падаем быстро, если что-то не так), чтобы не тратить время на сборку того, что не запустится.

Агенту кодирования передашь этот документ. Он структурирован так, чтобы проверять каждый болтик перед сборкой машины.

---

# ДЕТАЛЬНЫЙ ПЛАН РАЗРАБОТКИ POC (macOS / Apple Silicon)

**Цель:** Собрать локального мультимодального ассистента (Voice + Vision + RAG) на базе MLX.
**Принципы:**

1. Сначала нативные легкие компоненты (OCR, Embeddings).
2. Тяжелая артиллерия (Qwen) — в последнюю очередь.
3. Строгий контроль памяти (избегаем свопа на 8/16 ГБ).

---

## ЭТАП 0: ПОДГОТОВКА ОКРУЖЕНИЯ (Environment)

*Задача: Создать чистый фундамент, исключить конфликты версий.*

1. **Установка Python:** Строго 3.10 или 3.11 (самые стабильные для MLX).
2. **Установка зависимостей (строго по списку из отчета):**
    * [cite_start]`mlx`, `mlx-lm`, `mlx-vlm` [для Qwen](cite: 19, 140).
    * [cite_start]`pyobjc-framework-Vision`, `pyobjc-framework-Quartz` [для нативного OCR](cite: 141).
    * `numpy` (стандарт).
    * `lightning-whisper-mlx` (для распознавания речи).

**Скрипт проверки (`00_check_env.py`):**

* Вывести версию Python.
* Импортировать `mlx.core` и проверить `mx.metal.is_available()` (должно быть True).
* Проверить доступную Unified Memory.

---

## ЭТАП 1: МОДУЛЬНЫЕ ТЕСТЫ (Unit Testing)

*Задача: Проверить каждый компонент в изоляции. Если скрипт падает — дальше не идем.*

### 1.1. OCR (Apple Vision Framework)

*Риск:* Не распознает кириллицу или требует Swift.

* **Скрипт:** `01_test_ocr.py`
* **Действия:**
  * [cite_start]Использовать `VNRecognizeTextRequest` через `pyobjc`[cite: 98, 107].
  * [cite_start]**ВАЖНО:** Установить `.setRecognitionLevel_(Vision.VNRequestTextRecognitionLevelAccurate)`[cite: 114].
  * [cite_start]**ВАЖНО:** Явно задать языки `['ru-RU', 'en-US']`[cite: 116].
* **Тест:** Скормить картинку с русским текстом.
* **Критерий успеха:** Вывод текста в консоль < 0.5 сек. [cite_start]Потребление RAM < 100 МБ[cite: 101].

### 1.2. Embeddings (BGE-M3)

*Риск:* Утечка памяти на MPS (PyTorch).

* **Скрипт:** `02_test_embeddings.py`
* **Действия:**
  * [cite_start]Использовать библиотеку `mlx-embeddings` или порт `mlx-lm` [feature extraction](cite: 78).
  * [cite_start]Загрузить `BAAI/bge-m3` [желательно 8-bit или 4-bit, чтобы уложиться в 0.6 ГБ](cite: 86).
* **Тест:** Векторизовать текст длиной 500 слов.
* **Критерий успеха:** Вектор получен, RAM процесса не растет при повторных запусках.

### 1.3. Whisper (Speech-to-Text)

*Риск:* Медленная работа.

* **Скрипт:** `03_test_whisper.py`
* **Действия:**
  * Загрузить `lightning-whisper-mlx` (модель `large-v3-turbo`).
* **Тест:** Записать 5 секунд с микрофона -> транскрибация.
* **Критерий успеха:** Точность распознавания, скорость > 10x real-time.

---

## ЭТАП 2: "ЛЕГКОЕ ЯДРО" (OCR + Embeddings + Search)

*Задача: Собрать приложение "Вторая память" БЕЗ тяжелой LLM. Это база для RAG.*

**Архитектура скрипта (`poc_light_core.py`):**

1. **Вход:** Сделать скриншот экрана (через `screencapture` или Quartz).
2. **Обработка:**
    * Передать скриншот в **OCR**. Получить текст.
    * Передать текст в **Embedder**. Получить вектор.
3. **Хранение:** Сохранить `{timestamp, text, vector}` в локальный файл (JSON или простая vectordb типа LanceDB/Chroma, но для POC хватит JSON + numpy).
4. **Поиск:** Функция `search(query)`, которая берет запрос, делает эмбеддинг и ищет ближайший сохраненный скриншот (Cosine Similarity).

**E2E Тест:**

1. Открываешь статью на Хабре/Wiki.
2. Запускаешь скрипт (он "запоминает").
3. Пишешь запрос "О чем была статья?".
4. Скрипт выдает кусок текста из статьи.
*Если на этом этапе память течет или Mac греется — стоп. Оптимизируем.*

---

## ЭТАП 3: ИНТЕГРАЦИЯ QWEN3-VL (The Heavy Lifter)

*Задача: Добавить "мозги", которые будут анализировать найденное.*

**Скрипт (`04_test_qwen.py`):**

* **Действия:**
  * [cite_start]Использовать `mlx_vlm`[cite: 49].
  * [cite_start]Загрузить модель `mlx-community/Qwen3-VL-4B-Instruct-4bit`[cite: 39].
  * [cite_start]**ВАЖНО:** Проверить, что модель корректно принимает картинки (`process_vision_info`)[cite: 52].
* **Тест:** Загрузить модель + картинку -> Генерация описания.
* [cite_start]**Критерий успеха:** Скорость генерации > 20 токенов/сек[cite: 64].

---

## ЭТАП 4: ФИНАЛЬНЫЙ POC (Full Loop)

*Задача: Объединить всё в один цикл.*

**Логика приложения (`app.py`):**

1. **Start:** Загружаем Whisper и Qwen (мониторим RAM: должно быть < 6-7 ГБ).
2. **Listener:** Ждем Voice Command (например, "Смотри").
3. **Action:**
    * Делаем скриншот.
    * (Параллельно) Whisper переводит голос в текст промпта.
4. **Processing:**
    * Если вопрос по картинке -> Скриншот + Текст отправляем в **Qwen**.
    * Если вопрос по истории -> Текст -> **Embedder** -> Поиск в базе -> Контекст в **Qwen**.
5. **Output:** Печатаем ответ (или озвучиваем системным TTS).

---

### Чек-лист для Агента Кодирования (Definition of Done)

1. [ ] **01_ocr.py**: Работает на Mac без Docker/Swfit, понимает русский.
2. [ ] **02_embed.py**: Не использует PyTorch MPS backend, использует MLX.
3. [ ] **03_rag.py**: Ищет по базе векторов за < 0.1 сек.
4. [ ] **04_qwen.py**: Грузит модель 4-bit, потребляет < 4GB RAM.
5. [ ] **app.py**: Не падает с OOM (Out Of Memory) после 10 запросов подряд.

Володя, этот план отсекает все "повороты не туда". Если Агент споткнется на OCR или Embeddings — мы узнаем это через 10 минут, а не через 4 часа.
