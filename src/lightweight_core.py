"""
Lightweight Core - "–í—Ç–æ—Ä–∞—è –ø–∞–º—è—Ç—å".

–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç:
- Screenshot capture (Quartz)
- OCR (Apple Vision)
- Embeddings (MLX)
- Vector Storage
- Semantic Search

–ë–µ–∑ —Ç—è–∂–µ–ª—ã—Ö LLM - —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–ª—è POC.
"""

import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º
sys.path.insert(0, str(Path(__file__).parent))

from screenshot import capture_screenshot
from storage import VectorStorage
from search import VectorSearch
import Vision
import Quartz
from Foundation import NSURL
import mlx.core as mx
from mlx_embeddings.utils import load
import numpy as np


class LightweightCore:
    """–õ–µ–≥–∫–æ–≤–µ—Å–Ω–æ–µ —è–¥—Ä–æ –¥–ª—è –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏—è –∏ –ø–æ–∏—Å–∫–∞ —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤."""

    def __init__(self, storage_dir: Path = None):
        """
        Args:
            storage_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö.
        """
        if storage_dir is None:
            storage_dir = Path(__file__).parent.parent / "memory_storage"

        self.storage = VectorStorage(storage_dir)

        # –õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π (–∑–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏)
        self._embedding_model = None
        self._tokenizer = None

        print("‚úÖ Lightweight Core –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        print(f"   üìÅ –•—Ä–∞–Ω–∏–ª–∏—â–µ: {storage_dir}")
        print(f"   üìä –ó–∞–ø–∏—Å–µ–π –≤ –±–∞–∑–µ: {self.storage.count()}")

    def _get_embedding_model(self):
        """–õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤."""
        if self._embedding_model is None:
            print("\nüì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
            self._embedding_model, self._tokenizer = load("mlx-community/all-MiniLM-L6-v2-4bit")
            print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        return self._embedding_model, self._tokenizer

    def _ocr_recognize(self, image_path: str) -> str:
        """
        –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ Apple Vision.

        Args:
            image_path: –ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é

        Returns:
            –†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        input_url = NSURL.fileURLWithPath_(image_path)
        ci_image = Quartz.CIImage.imageWithContentsOfURL_(input_url)

        if not ci_image:
            raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {image_path}")

        # –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∫–≤–µ—Å—Ç–∞ OCR
        request = Vision.VNRecognizeTextRequest.alloc().init()
        request.setRecognitionLevel_(Vision.VNRequestTextRecognitionLevelAccurate)
        request.setRecognitionLanguages_(["ru-RU", "en-US"])
        request.setUsesLanguageCorrection_(True)

        # –û–±—Ä–∞–±–æ—Ç—á–∏–∫
        handler = Vision.VNImageRequestHandler.alloc().initWithCIImage_options_(
            ci_image, None
        )

        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
        success = handler.performRequests_error_([request], None)

        if not success:
            return ""

        # –°–±–æ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        observations = request.results()
        if not observations:
            return ""

        texts = [obs.text() for obs in observations]
        return "\n".join(texts)

    def _generate_embedding(self, text: str) -> np.ndarray:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞.

        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏

        Returns:
            Numpy array —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–º
        """
        model, tokenizer = self._get_embedding_model()
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        inputs = tokenizer.batch_encode_plus(
            [text],
            return_tensors="mlx",
            padding=True,
            truncation=True,
            max_length=512,  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        )
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
        outputs = model(inputs["input_ids"], attention_mask=inputs["attention_mask"])
        embeddings = outputs.text_embeds
        
        return np.array(embeddings[0])

    def remember(self, screenshot_path: str = None) -> int:
        """
        –ó–∞–ø–æ–º–Ω–∏—Ç—å —Å–∫—Ä–∏–Ω—à–æ—Ç (–∑–∞—Ö–≤–∞—Ç–∏—Ç—å + OCR + embedding + —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å).

        Args:
            screenshot_path: –ü—É—Ç—å –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º—É —Å–∫—Ä–∏–Ω—à–æ—Ç—É.
                           –ï—Å–ª–∏ None, –¥–µ–ª–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π —Å–∫—Ä–∏–Ω—à–æ—Ç.

        Returns:
            ID —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –∑–∞–ø–∏—Å–∏
        """
        print("\n" + "=" * 70)
        print("üì∏ –ó–ê–ü–û–ú–ò–ù–ê–ù–ò–ï")
        print("=" * 70)

        # –®–∞–≥ 1: –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–∫—Ä–∏–Ω—à–æ—Ç–∞
        if screenshot_path is None:
            print("\n1Ô∏è‚É£  –ó–∞—Ö–≤–∞—Ç —Å–∫—Ä–∏–Ω—à–æ—Ç–∞...")
            screenshot_path = capture_screenshot()
        else:
            print(f"\n1Ô∏è‚É£  –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Å–∫—Ä–∏–Ω—à–æ—Ç–∞: {screenshot_path}")

        # –®–∞–≥ 2: OCR
        print("\n2Ô∏è‚É£  –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ (OCR)...")
        text = self._ocr_recognize(str(screenshot_path))
        print(f"   ‚úÖ –†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
        if len(text) > 100:
            print(f"   Preview: {text[:100]}...")
        else:
            print(f"   –¢–µ–∫—Å—Ç: {text}")

        if not text.strip():
            print("   ‚ö†Ô∏è  –¢–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏")
            return -1

        # –®–∞–≥ 3: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
        print("\n3Ô∏è‚É£  –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞...")
        vector = self._generate_embedding(text)
        print(f"   ‚úÖ –í–µ–∫—Ç–æ—Ä —Å–æ–∑–¥–∞–Ω: —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å {vector.shape}")

        # –®–∞–≥ 4: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        print("\n4Ô∏è‚É£  –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—É...")
        record_id = self.storage.add(
            vector=vector, text=text, screenshot_path=str(screenshot_path)
        )

        print("\n" + "=" * 70)
        print(f"‚úÖ –ó–ê–ü–û–ú–ù–ï–ù–û! Record ID: {record_id}")
        print("=" * 70)

        return record_id

    def search(self, query: str, top_k: int = 5) -> list:
        """
        –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –∑–∞–ø—Ä–æ—Å—É.

        Args:
            query: –¢–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

        Returns:
            List[(metadata, similarity_score)]
        """
        print("\n" + "=" * 70)
        print(f"üîç –ü–û–ò–°–ö: \"{query}\"")
        print("=" * 70)

        if self.storage.count() == 0:
            print("\n‚ùå –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –ø—É—Å—Ç–∞. –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø–æ–º–Ω–∏—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤.")
            return []

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∑–∞–ø—Ä–æ—Å–∞
        print("\n1Ô∏è‚É£  –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞...")
        query_vector = self._generate_embedding(query)

        # –ü–æ–∏—Å–∫
        print("\n2Ô∏è‚É£  –ü–æ–∏—Å–∫ –≤ –±–∞–∑–µ...")
        results = VectorSearch.search(
            query_vector=query_vector,
            database_vectors=self.storage.get_all_vectors(),
            database_metadata=self.storage.get_all_metadata(),
            top_k=top_k,
            threshold=0.3,  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ 30%
        )

        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print(VectorSearch.format_results(results))

        return results


if __name__ == "__main__":
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã
    print("\n" + "üß™" * 35)
    print("–î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø LIGHTWEIGHT CORE")
    print("üß™" * 35)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    core = LightweightCore()

    # –¢–µ—Å—Ç 1: –ó–∞–ø–æ–º–∏–Ω–∞–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤ –∏–∑ test_images
    test_images = Path(__file__).parent.parent / "test_images"
    if test_images.exists():
        images = list(test_images.glob("*.png")) + list(test_images.glob("*.jpg"))
        print(f"\nüìÅ –ù–∞–π–¥–µ–Ω–æ {len(images)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")

        for img in images[:2]:  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 2 –¥–ª—è –¥–µ–º–æ
            core.remember(str(img))

    # –¢–µ—Å—Ç 2: –ü–æ–∏—Å–∫
    if core.storage.count() > 0:
        # –ü–æ–∏—Å–∫ –ø–æ —Ä–∞–∑–Ω—ã–º –∑–∞–ø—Ä–æ—Å–∞–º
        test_queries = [
            "Python",
            "GLM API",
            "–æ—à–∏–±–∫–∞",
        ]

        for query in test_queries:
            core.search(query, top_k=2)

    print("\n‚úÖ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
