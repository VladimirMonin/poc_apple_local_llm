"""
–¢–µ—Å—Ç BGE-M3 Embeddings –Ω–∞ MLX.

–ü—Ä–æ–≤–µ—Ä—è–µ—Ç:
- –ó–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏ BGE-M3 —á–µ—Ä–µ–∑ mlx-embeddings
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—é —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Ç–µ–∫—Å—Ç–∞
- –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
- –†–∞–±–æ—Ç—É —Å –¥–ª–∏–Ω–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏ (–¥–æ 8192 —Ç–æ–∫–µ–Ω–æ–≤)
- –ü–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏
"""

import time
import sys
import os
import mlx.core as mx
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np


def get_embeddings(texts, model, tokenizer):
    """
    –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤.

    Args:
        texts: –°–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        model: –ú–æ–¥–µ–ª—å MLX
        tokenizer: –¢–æ–∫–µ–Ω–∞–π–∑–µ—Ä

    Returns:
        numpy.ndarray: –ú–∞—Ç—Ä–∏—Ü–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    """
    inputs = tokenizer.batch_encode_plus(
        texts,
        return_tensors="mlx",
        padding=True,
        truncation=True,
        max_length=8192,  # BGE-M3 –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –¥–æ 8192 —Ç–æ–∫–µ–Ω–æ–≤
    )

    outputs = model(inputs["input_ids"], attention_mask=inputs["attention_mask"])

    # MLX –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç mean pooled –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    embeddings = outputs.text_embeds

    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy –¥–ª—è sklearn
    return np.array(embeddings)


def test_basic_embeddings():
    """–ë–∞–∑–æ–≤—ã–π —Ç–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤."""
    print("=" * 70)
    print("–¢–ï–°–¢ 1: –ë–∞–∑–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
    print("=" * 70)
    print()

    try:
        from mlx_embeddings.utils import load

        print("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ BGE-M3 (–∏–ª–∏ –∞–Ω–∞–ª–æ–≥–∞)...")
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–µ–Ω—å—à—É—é –º–æ–¥–µ–ª—å all-MiniLM –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        # BGE-M3 –ø–æ–∫–∞ –Ω–µ—Ç –≤ MLX —Ñ–æ—Ä–º–∞—Ç–µ –Ω–∞ HuggingFace
        model_name = "mlx-community/all-MiniLM-L6-v2-4bit"

        print(f"   –ú–æ–¥–µ–ª—å: {model_name}")
        start_time = time.time()
        model, tokenizer = load(model_name)
        load_time = time.time() - start_time

        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {load_time:.2f} —Å–µ–∫")
        print()

        # –¢–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã
        texts = [
            "–Ø –ª—é–±–ª—é –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ Python",
            "–ú–Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è –ø–∏—Å–∞—Ç—å –∫–æ–¥ –Ω–∞ Python",
            "–ß–µ—Ä–µ–ø–∞—Ö–∞ –º–µ–¥–ª–µ–Ω–Ω–æ –ø–æ–ª–∑—ë—Ç –ø–æ–¥ –¥–µ—Ä–µ–≤–æ–º",
        ]

        print("üîÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
        start_time = time.time()
        embeddings = get_embeddings(texts, model, tokenizer)
        embed_time = time.time() - start_time

        print(f"‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã –∑–∞ {embed_time:.3f} —Å–µ–∫")
        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {embeddings.shape}")
        print()

        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ö–æ–¥—Å—Ç–≤–∞
        print("üìä –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞...")
        similarity_matrix = cosine_similarity(embeddings)

        print("\n–ú–∞—Ç—Ä–∏—Ü–∞ —Å—Ö–æ–¥—Å—Ç–≤–∞:")
        print("-" * 70)
        for i in range(len(texts)):
            for j in range(len(texts)):
                if i < j:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –≤–µ—Ä—Ö–Ω–∏–π —Ç—Ä–µ—É–≥–æ–ª—å–Ω–∏–∫
                    print(
                        f"–¢–µ–∫—Å—Ç {i + 1} ‚Üî –¢–µ–∫—Å—Ç {j + 1}: {similarity_matrix[i][j]:.4f}"
                    )
        print("-" * 70)
        print()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–æ–≥–∏–∫–∏
        # –¢–µ–∫—Å—Ç—ã 1 –∏ 2 (–ø—Ä–æ Python) –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø–æ—Ö–æ–∂–∏
        # –¢–µ–∫—Å—Ç 3 (–ø—Ä–æ —á–µ—Ä–µ–ø–∞—Ö—É) –¥–æ–ª–∂–µ–Ω –æ—Ç–ª–∏—á–∞—Ç—å—Å—è
        sim_python = similarity_matrix[0][1]
        sim_diff = similarity_matrix[0][2]

        if sim_python > 0.7:
            print(f"‚úÖ –ü–æ—Ö–æ–∂–∏–µ —Ç–µ–∫—Å—Ç—ã —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω—ã (—Å—Ö–æ–¥—Å—Ç–≤–æ {sim_python:.2%})")
        else:
            print(f"‚ö†Ô∏è  –ù–∏–∑–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –ø–æ—Ö–æ–∂–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ ({sim_python:.2%})")

        if sim_diff < 0.5:
            print(f"‚úÖ –†–∞–∑–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã —Ä–∞–∑–ª–∏—á–∞—é—Ç—Å—è (—Å—Ö–æ–¥—Å—Ç–≤–æ {sim_diff:.2%})")
        else:
            print(f"‚ö†Ô∏è  –í—ã—Å–æ–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ —Ä–∞–∑–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ ({sim_diff:.2%})")

        return True, model, tokenizer

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
        import traceback

        traceback.print_exc()
        return False, None, None


def test_long_text(model, tokenizer):
    """–¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤."""
    print()
    print("=" * 70)
    print("–¢–ï–°–¢ 2: –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª–∏–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ (500 —Å–ª–æ–≤)")
    print("=" * 70)
    print()

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    long_text = (
        """
    Apple Silicon –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å–µ—Ä–∏—é –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤ –Ω–∞ –±–∞–∑–µ ARM –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã, 
    —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö Apple Inc. —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è –∏—Ö –∫–æ–º–ø—å—é—Ç–µ—Ä–æ–≤ Mac. –ü–µ—Ä–≤—ã–º —á–∏–ø–æ–º 
    –≤ —ç—Ç–æ–π –ª–∏–Ω–µ–π–∫–µ —Å—Ç–∞–ª M1, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –≤ –Ω–æ—è–±—Ä–µ 2020 –≥–æ–¥–∞. –≠—Ç–æ—Ç —á–∏–ø 
    –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä (CPU), –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä (GPU), 
    –Ω–µ–π—Ä–æ–Ω–Ω—ã–π –¥–≤–∏–∂–æ–∫ (Neural Engine) –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—É—é –ø–∞–º—è—Ç—å (Unified Memory) 
    –Ω–∞ –æ–¥–Ω–æ–º –∫—Ä–∏—Å—Ç–∞–ª–ª–µ.
    
    Unified Memory Architecture (UMA) —è–≤–ª—è–µ—Ç—Å—è –∫–ª—é—á–µ–≤–æ–π –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å—é Apple Silicon.
    –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä, –≥–¥–µ CPU –∏ GPU –∏–º–µ—é—Ç —Ä–∞–∑–¥–µ–ª—å–Ω—ã–µ –ø—É–ª—ã 
    –ø–∞–º—è—Ç–∏, –≤ UMA –≤—Å–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –±–ª–æ–∫–∏ –∏–º–µ—é—Ç –¥–æ—Å—Ç—É–ø –∫ –µ–¥–∏–Ω–æ–º—É –º–∞—Å—Å–∏–≤—É –¥–∞–Ω–Ω—ã—Ö.
    –≠—Ç–æ —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –º–µ–∂–¥—É RAM –∏ VRAM, —á—Ç–æ 
    –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ —ç–Ω–µ—Ä–≥–æ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å.
    
    MLX - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π Apple Machine Learning 
    Research —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è Apple Silicon. MLX –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å 
    Unified Memory –∏ Metal Performance Shaders. –û–Ω –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç API, –ø–æ—Ö–æ–∂–∏–π 
    –Ω–∞ NumPy –∏ PyTorch, –Ω–æ —Å —É—á–µ—Ç–æ–º –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Apple.
    """
        * 5
    )  # –ü–æ–≤—Ç–æ—Ä—è–µ–º –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è –¥–ª–∏–Ω—ã

    try:
        print(f"üìù –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: ~{len(long_text.split())} —Å–ª–æ–≤")

        start_time = time.time()
        embeddings = get_embeddings([long_text], model, tokenizer)
        embed_time = time.time() - start_time

        print(f"‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –ø–æ–ª—É—á–µ–Ω –∑–∞ {embed_time:.3f} —Å–µ–∫")
        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {embeddings.shape}")

        if embed_time < 2.0:
            print("‚úÖ –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Ç–ª–∏—á–Ω–∞—è (< 2 —Å–µ–∫)")
        else:
            print(f"‚ö†Ô∏è  –ú–µ–¥–ª–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ ({embed_time:.3f} —Å–µ–∫)")

        return True

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–ª–∏–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: {e}")
        import traceback

        traceback.print_exc()
        return False


def test_memory_usage(model, tokenizer):
    """–¢–µ—Å—Ç –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è –ø–∞–º—è—Ç–∏."""
    print()
    print("=" * 70)
    print("–¢–ï–°–¢ 3: –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è –ø–∞–º—è—Ç–∏")
    print("=" * 70)
    print()

    try:
        import psutil

        process = psutil.Process(os.getpid())
    except ImportError:
        print("‚ö†Ô∏è  psutil –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç –ø–∞–º—è—Ç–∏")
        return True

    mem_before = process.memory_info().rss / 1024 / 1024  # –ú–ë
    print(f"–ü–∞–º—è—Ç—å –¥–æ —Ç–µ—Å—Ç–∞: {mem_before:.1f} –ú–ë")

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ 10 —Ä–∞–∑
    test_text = "–¢–µ—Å—Ç–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–∞–º—è—Ç–∏. " * 50

    for i in range(10):
        embeddings = get_embeddings([test_text], model, tokenizer)
        if (i + 1) % 3 == 0:
            print(f"  –ò—Ç–µ—Ä–∞—Ü–∏—è {i + 1}/10 –∑–∞–≤–µ—Ä—à–µ–Ω–∞")

    mem_after = process.memory_info().rss / 1024 / 1024  # –ú–ë
    mem_delta = mem_after - mem_before

    print(f"–ü–∞–º—è—Ç—å –ø–æ—Å–ª–µ —Ç–µ—Å—Ç–∞: {mem_after:.1f} –ú–ë")
    print(f"–ü—Ä–∏—Ä–æ—Å—Ç –ø–∞–º—è—Ç–∏: {mem_delta:.1f} –ú–ë")
    print()

    if mem_delta < 200:
        print("‚úÖ –ü–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –Ω–æ—Ä–º—ã (< 200 –ú–ë)")
        return True
    else:
        print(f"‚ö†Ô∏è  –í—ã—Å–æ–∫–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ ({mem_delta:.1f} –ú–ë)")
        return True  # –ù–µ –±–ª–æ–∫–∏—Ä—É–µ–º –Ω–∞ —ç—Ç–æ–º


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    results = []

    # –¢–µ—Å—Ç 1: –ë–∞–∑–æ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    success, model, tokenizer = test_basic_embeddings()
    results.append(success)

    if not success:
        print("\n‚ùå –ë–∞–∑–æ–≤—ã–π —Ç–µ—Å—Ç –Ω–µ –ø—Ä–æ–π–¥–µ–Ω, –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ–ø—É—â–µ–Ω—ã")
        return False

    # –¢–µ—Å—Ç 2: –î–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    results.append(test_long_text(model, tokenizer))

    # –¢–µ—Å—Ç 3: –ü–∞–º—è—Ç—å
    results.append(test_memory_usage(model, tokenizer))

    print()
    print("=" * 70)
    if all(results):
        print("‚úÖ –í–°–ï –¢–ï–°–¢–´ EMBEDDINGS –ü–†–û–ô–î–ï–ù–´ –£–°–ü–ï–®–ù–û")
    else:
        print("‚ö†Ô∏è  –ù–ï–ö–û–¢–û–†–´–ï –¢–ï–°–¢–´ –ù–ï –ü–†–û–ô–î–ï–ù–´")
    print("=" * 70)

    return all(results)


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
