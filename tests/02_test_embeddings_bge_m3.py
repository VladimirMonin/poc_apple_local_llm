"""
–¢–µ—Å—Ç BGE-M3 Embeddings (–∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –∏–∑ poc_plan.md).

–ü—Ä–æ–≤–µ—Ä—è–µ—Ç:
- –ó–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏ BAAI/bge-m3 —á–µ—Ä–µ–∑ mlx-embeddings
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—é —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞:
  * –†—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç
  * –ê–Ω–≥–ª–∏–π—Å–∫–∏–π —Ç–µ–∫—Å—Ç
  * –ë–ª–æ–∫–∏ –∫–æ–¥–∞
- –ö–∞—á–µ—Å—Ç–≤–æ (cosine similarity –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø–∞—Ä–∞—Ö)
- –ü–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤ (RAM, swap, timing)
- –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å all-MiniLM-L6-v2-4bit

–¶–µ–ª—å: –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –ø–æ—á–µ–º—É –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å.
"""

import time
import sys
import os
import mlx.core as mx
import numpy as np
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ PYTHONPATH
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.profiler import SystemProfiler


def get_embeddings_mlx(texts, model, tokenizer):
    """
    –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —á–µ—Ä–µ–∑ MLX.

    Args:
        texts: –°–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        model: –ú–æ–¥–µ–ª—å MLX
        tokenizer: –¢–æ–∫–µ–Ω–∞–π–∑–µ—Ä

    Returns:
        numpy.ndarray: –ú–∞—Ç—Ä–∏—Ü–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    """
    inputs = tokenizer.batch_encode_plus(
        texts,
        return_tensors="mlx",
        padding=True,
        truncation=True,
        max_length=8192,  # BGE-M3 –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –¥–ª–∏–Ω–Ω—ã–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã
    )

    outputs = model(inputs["input_ids"], attention_mask=inputs["attention_mask"])
    embeddings = outputs.text_embeds

    return np.array(embeddings)


def cosine_similarity_manual(vec1, vec2):
    """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ –≤—Ä—É—á–Ω—É—é."""
    dot_product = np.dot(vec1, vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    return dot_product / (norm1 * norm2)


def test_model_loading():
    """–¢–µ—Å—Ç –∑–∞–≥—Ä—É–∑–∫–∏ BGE-M3 —Å –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º."""
    print("=" * 80)
    print("–¢–ï–°–¢ 1: –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò BGE-M3")
    print("=" * 80)
    print()

    profiler = SystemProfiler()

    try:
        from mlx_embeddings.utils import load

        # –ò—â–µ–º BGE-M3 –≤ MLX —Ñ–æ—Ä–º–∞—Ç–µ
        # –í–æ–∑–º–æ–∂–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã:
        # 1. mlx-community/bge-m3-4bit (–µ—Å–ª–∏ –µ—Å—Ç—å)
        # 2. mlx-community/bge-m3 (–µ—Å–ª–∏ –µ—Å—Ç—å)
        # 3. BAAI/bge-m3 (–æ—Ä–∏–≥–∏–Ω–∞–ª, –º–æ–∂–µ—Ç –Ω–µ –±—ã—Ç—å MLX —Ñ–æ—Ä–º–∞—Ç–∞)

        model_candidates = [
            "mlx-community/bge-m3-4bit",
            "mlx-community/bge-m3",
            "mlx-community/bge-large-en-v1.5-4bit",
            "mlx-community/bge-small-en-v1.5-4bit",  # –î–û–ö–ê–ó–ê–ù–û: —Ä–∞–±–æ—Ç–∞–µ—Ç!
            "BAAI/bge-m3",
        ]

        model = None
        tokenizer = None
        model_name = None

        print("üîç –ü–æ–∏—Å–∫ BGE-M3 –≤ MLX —Ñ–æ—Ä–º–∞—Ç–µ...")
        print()

        for candidate in model_candidates:
            print(f"   –ü—Ä–æ–±—É–µ–º: {candidate}")
            try:
                start_time = time.time()
                model, tokenizer = load(candidate)
                load_time = time.time() - start_time
                model_name = candidate
                print(f"   ‚úÖ –£—Å–ø–µ—Ö! –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞ {load_time:.2f} —Å–µ–∫")
                break
            except Exception as e:
                print(f"   ‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ: {str(e)[:100]}")

        if model is None:
            print()
            print("‚ö†Ô∏è  BGE-M3 –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ MLX —Ñ–æ—Ä–º–∞—Ç–µ –Ω–∞ HuggingFace!")
            print()
            print("–í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
            print("1. –ú–æ–¥–µ–ª—å –µ—â–µ –Ω–µ –ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞ –≤ MLX —Ñ–æ—Ä–º–∞—Ç")
            print("2. –¢—Ä–µ–±—É–µ—Ç—Å—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –≤—Ä—É—á–Ω—É—é")
            print("3. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π PyTorch —Ñ–æ—Ä–º–∞—Ç (–∫–æ–Ω—Ñ–ª–∏–∫—Ç —Å –∫–æ–Ω—Ü–µ–ø—Ü–∏–µ–π)")
            print()
            print("–û–¥–Ω–∞–∫–æ –ù–ê–ô–î–ï–ù–ê: bge-small-en-v1.5-4bit (–º–µ–Ω—å—à–∞—è –≤–µ—Ä—Å–∏—è BGE)")
            print("–≠—Ç–æ –æ–±—ä—è—Å–Ω—è–µ—Ç, –ø–æ—á–µ–º—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ all-MiniLM-L6-v2-4bit.")
            return False, None, None, None

        mem_snapshot = profiler._get_memory_snapshot()

        print()
        print("üìä –ú–ï–¢–†–ò–ö–ò –ó–ê–ì–†–£–ó–ö–ò:")
        print("-" * 80)
        print(f"   –ú–æ–¥–µ–ª—å: {model_name}")
        print(f"   –í—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏: {load_time:.2f} —Å–µ–∫")
        print(f"   RAM –ø—Ä–æ—Ü–µ—Å—Å–∞: {mem_snapshot.rss_mb:.1f} MB")
        print(f"   Swap: {mem_snapshot.swap_used_mb:.1f} MB")
        print("-" * 80)

        return True, model, tokenizer, model_name

    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback

        traceback.print_exc()
        return False, None, None, None


def test_russian_text(model, tokenizer, model_name):
    """–¢–µ—Å—Ç –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —Ç–µ–∫—Å—Ç–µ."""
    print()
    print("=" * 80)
    print("–¢–ï–°–¢ 2: –ö–ê–ß–ï–°–¢–í–û –ù–ê –†–£–°–°–ö–û–ú –¢–ï–ö–°–¢–ï")
    print("=" * 80)
    print()

    # –¢–µ—Å—Ç–æ–≤—ã–µ –ø–∞—Ä—ã: –ø–æ—Ö–æ–∂–∏–µ –∏ –Ω–µ–ø–æ—Ö–æ–∂–∏–µ
    test_pairs = [
        {
            "name": "–ü–æ—Ö–æ–∂–∏–µ —Ç–µ–∫—Å—Ç—ã (–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ)",
            "text1": "–Ø –ª—é–±–ª—é –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ Python –∏ —Å–æ–∑–¥–∞–≤–∞—Ç—å –≤–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è",
            "text2": "–ú–Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è –ø–∏—Å–∞—Ç—å –∫–æ–¥ –Ω–∞ Python –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ —Å–∞–π—Ç–æ–≤",
            "expected_similarity": "–≤—ã—Å–æ–∫–æ–µ (>0.7)",
        },
        {
            "name": "–ü–æ—Ö–æ–∂–∏–µ —Ç–µ–∫—Å—Ç—ã (ML)",
            "text1": "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á",
            "text2": "Neural networks –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –≤ machine learning –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ü–µ–ª–µ–π",
            "expected_similarity": "–≤—ã—Å–æ–∫–æ–µ (>0.6)",
        },
        {
            "name": "–†–∞–∑–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã",
            "text1": "–°–µ–≥–æ–¥–Ω—è —Å–æ–ª–Ω–µ—á–Ω–∞—è –ø–æ–≥–æ–¥–∞, –ø—Ç–∏—Ü—ã –ø–æ—é—Ç –≤ —Å–∞–¥—É",
            "text2": "–ö–≤–∞–Ω—Ç–æ–≤–∞—è —Ñ–∏–∑–∏–∫–∞ –∏–∑—É—á–∞–µ—Ç –ø–æ–≤–µ–¥–µ–Ω–∏–µ —á–∞—Å—Ç–∏—Ü –Ω–∞ –∞—Ç–æ–º–Ω–æ–º —É—Ä–æ–≤–Ω–µ",
            "expected_similarity": "–Ω–∏–∑–∫–æ–µ (<0.3)",
        },
    ]

    results = []

    profiler = SystemProfiler()

    for pair in test_pairs:
        print(f"üìù {pair['name']}")
        print(f"   –¢–µ–∫—Å—Ç 1: {pair['text1'][:60]}...")
        print(f"   –¢–µ–∫—Å—Ç 2: {pair['text2'][:60]}...")

        start_time = time.time()
        embeddings = get_embeddings_mlx(
            [pair["text1"], pair["text2"]], model, tokenizer
        )
        embed_time = time.time() - start_time

        similarity = cosine_similarity_manual(embeddings[0], embeddings[1])

        print(
            f"   –°—Ö–æ–¥—Å—Ç–≤–æ: {similarity:.4f} (–æ–∂–∏–¥–∞–ª–æ—Å—å {pair['expected_similarity']})"
        )
        print(f"   –í—Ä–µ–º—è: {embed_time:.3f} —Å–µ–∫")
        print()

        results.append(
            {
                "name": pair["name"],
                "similarity": similarity,
                "time": embed_time,
                "embedding_dim": embeddings.shape[1],
            }
        )

    mem_snapshot = profiler._get_memory_snapshot()

    print("üìä –°–í–û–î–ö–ê –ü–û –†–£–°–°–ö–û–ú–£ –¢–ï–ö–°–¢–£:")
    print("-" * 80)
    print(f"   –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å: {np.mean([r['time'] for r in results]):.3f} —Å–µ–∫")
    print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤: {results[0]['embedding_dim']}")
    print(f"   RAM: {mem_snapshot.rss_mb:.1f} MB")
    print(f"   Swap: {mem_snapshot.swap_used_mb:.1f} MB")
    print("-" * 80)

    return results


def test_english_text(model, tokenizer, model_name):
    """–¢–µ—Å—Ç –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —Ç–µ–∫—Å—Ç–µ."""
    print()
    print("=" * 80)
    print("–¢–ï–°–¢ 3: –ö–ê–ß–ï–°–¢–í–û –ù–ê –ê–ù–ì–õ–ò–ô–°–ö–û–ú –¢–ï–ö–°–¢–ï")
    print("=" * 80)
    print()

    test_pairs = [
        {
            "name": "Similar texts (AI/ML)",
            "text1": "Artificial intelligence and machine learning are transforming technology",
            "text2": "AI and ML technologies are revolutionizing the tech industry",
            "expected_similarity": "high (>0.7)",
        },
        {
            "name": "Similar texts (Apple Silicon)",
            "text1": "Apple Silicon uses unified memory architecture for better performance",
            "text2": "M-series chips from Apple leverage UMA to improve efficiency",
            "expected_similarity": "high (>0.6)",
        },
        {
            "name": "Different texts",
            "text1": "The weather is beautiful today with clear blue skies",
            "text2": "Database indexing improves query performance significantly",
            "expected_similarity": "low (<0.3)",
        },
    ]

    results = []

    profiler = SystemProfiler()

    for pair in test_pairs:
        print(f"üìù {pair['name']}")
        print(f"   Text 1: {pair['text1'][:60]}...")
        print(f"   Text 2: {pair['text2'][:60]}...")

        start_time = time.time()
        embeddings = get_embeddings_mlx(
            [pair["text1"], pair["text2"]], model, tokenizer
        )
        embed_time = time.time() - start_time

        similarity = cosine_similarity_manual(embeddings[0], embeddings[1])

        print(
            f"   Similarity: {similarity:.4f} (expected {pair['expected_similarity']})"
        )
        print(f"   Time: {embed_time:.3f} sec")
        print()

        results.append(
            {
                "name": pair["name"],
                "similarity": similarity,
                "time": embed_time,
            }
        )

    mem_snapshot = profiler._get_memory_snapshot()

    print("üìä ENGLISH TEXT SUMMARY:")
    print("-" * 80)
    print(f"   Average speed: {np.mean([r['time'] for r in results]):.3f} sec")
    print(f"   RAM: {mem_snapshot.rss_mb:.1f} MB")
    print(f"   Swap: {mem_snapshot.swap_used_mb:.1f} MB")
    print("-" * 80)

    return results


def test_code_blocks(model, tokenizer, model_name):
    """–¢–µ—Å—Ç –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ –±–ª–æ–∫–∞—Ö –∫–æ–¥–∞."""
    print()
    print("=" * 80)
    print("–¢–ï–°–¢ 4: –ö–ê–ß–ï–°–¢–í–û –ù–ê –ë–õ–û–ö–ê–• –ö–û–î–ê")
    print("=" * 80)
    print()

    code_pairs = [
        {
            "name": "–ü–æ—Ö–æ–∂–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ (Python)",
            "code1": """
def calculate_sum(numbers):
    total = 0
    for num in numbers:
        total += num
    return total
""",
            "code2": """
def sum_array(arr):
    result = 0
    for item in arr:
        result = result + item
    return result
""",
            "expected_similarity": "–≤—ã—Å–æ–∫–æ–µ (>0.7)",
        },
        {
            "name": "–ü–æ—Ö–æ–∂–∏–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ (—Ä–∞–∑–Ω—ã–µ —è–∑—ã–∫–∏)",
            "code1": """
# Python
class User:
    def __init__(self, name):
        self.name = name
""",
            "code2": """
// JavaScript
class User {
    constructor(name) {
        this.name = name;
    }
}
""",
            "expected_similarity": "—Å—Ä–µ–¥–Ω–µ–µ (>0.5)",
        },
        {
            "name": "–†–∞–∑–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏",
            "code1": """
SELECT * FROM users WHERE age > 18 ORDER BY name;
""",
            "code2": """
import matplotlib.pyplot as plt
plt.plot([1, 2, 3], [4, 5, 6])
plt.show()
""",
            "expected_similarity": "–Ω–∏–∑–∫–æ–µ (<0.4)",
        },
    ]

    results = []

    results = []

    profiler = SystemProfiler()

    for pair in code_pairs:
        print(f"üíª {pair['name']}")

        start_time = time.time()
        embeddings = get_embeddings_mlx(
            [pair["code1"], pair["code2"]], model, tokenizer
        )
        embed_time = time.time() - start_time

        similarity = cosine_similarity_manual(embeddings[0], embeddings[1])

        print(
            f"   –°—Ö–æ–¥—Å—Ç–≤–æ –∫–æ–¥–∞: {similarity:.4f} (–æ–∂–∏–¥–∞–ª–æ—Å—å {pair['expected_similarity']})"
        )
        print(f"   –í—Ä–µ–º—è: {embed_time:.3f} —Å–µ–∫")
        print()

        results.append(
            {
                "name": pair["name"],
                "similarity": similarity,
                "time": embed_time,
            }
        )

    mem_snapshot = profiler._get_memory_snapshot()
    print("-" * 80)
    print(f"   –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å: {np.mean([r['time'] for r in results]):.3f} —Å–µ–∫")
    print(f"   RAM: {mem_snapshot.rss_mb:.1f} MB")
    print(f"   Swap: {mem_snapshot.swap_used_mb:.1f} MB")
    print("-" * 80)

    return results


def print_final_comparison():
    """–í—ã–≤–æ–¥ –∏—Ç–æ–≥–æ–≤–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å all-MiniLM-L6-v2-4bit."""
    print()
    print("=" * 80)
    print("–ò–¢–û–ì–û–í–û–ï –°–†–ê–í–ù–ï–ù–ò–ï: BGE-M3 vs all-MiniLM-L6-v2-4bit")
    print("=" * 80)
    print()

    comparison_table = """
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ –ü–∞—Ä–∞–º–µ—Ç—Ä                ‚îÇ BGE-M3           ‚îÇ all-MiniLM-L6-v2-4bit   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤    ‚îÇ 1024             ‚îÇ 384                     ‚îÇ
‚îÇ –†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏           ‚îÇ ~2.2 GB (4-bit)  ‚îÇ ~150 MB (4-bit)         ‚îÇ
‚îÇ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ MLX           ‚îÇ ???              ‚îÇ ‚úÖ –ï—Å—Ç—å                 ‚îÇ
‚îÇ –°–∫–æ—Ä–æ—Å—Ç—å (1 —Ç–µ–∫—Å—Ç)      ‚îÇ ???              ‚îÇ ~0.5 —Å–µ–∫                ‚îÇ
‚îÇ RAM (–∑–∞–≥—Ä—É–∑–∫–∞)          ‚îÇ ???              ‚îÇ ~200 MB                 ‚îÇ
‚îÇ –ö–∞—á–µ—Å—Ç–≤–æ (—Ä—É—Å—Å–∫–∏–π)      ‚îÇ ???              ‚îÇ –•–æ—Ä–æ—à–µ–µ (0.7+ similar)  ‚îÇ
‚îÇ –ö–∞—á–µ—Å—Ç–≤–æ (–∞–Ω–≥–ª–∏–π—Å–∫–∏–π)   ‚îÇ ???              ‚îÇ –•–æ—Ä–æ—à–µ–µ (0.75+ similar) ‚îÇ
‚îÇ –ö–∞—á–µ—Å—Ç–≤–æ (–∫–æ–¥)          ‚îÇ ???              ‚îÇ –°—Ä–µ–¥–Ω–µ–µ (0.5+ similar)  ‚îÇ
‚îÇ –ú–∞–∫—Å. –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞   ‚îÇ 8192 —Ç–æ–∫–µ–Ω–æ–≤     ‚îÇ 512 —Ç–æ–∫–µ–Ω–æ–≤             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

–ö–õ–Æ–ß–ï–í–ê–Ø –ü–†–û–ë–õ–ï–ú–ê:
    ‚ùå BGE-M3 –º–æ–∂–µ—Ç –ù–ï –ë–´–¢–¨ –≤ MLX —Ñ–æ—Ä–º–∞—Ç–µ –Ω–∞ HuggingFace
    ‚ùå –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ PyTorch –≤–µ—Ä—Å–∏–∏ –ù–ê–†–£–®–ê–ï–¢ –∫–æ–Ω—Ü–µ–ø—Ü–∏—é "–ë–ï–ó —Ç–∞–Ω—Ü–µ–≤ —Å –±—É–±–Ω–æ–º"
    ‚ùå –¢—Ä–µ–±—É–µ—Ç—Å—è —Ä—É—á–Ω–∞—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –≤ MLX (—Å–ª–æ–∂–Ω–æ –¥–ª—è POC)

–ü–û–ß–ï–ú–£ –í–´–ë–†–ê–õ–ò all-MiniLM-L6-v2-4bit:
    ‚úÖ –ì–æ—Ç–æ–≤—ã–π MLX —Ñ–æ—Ä–º–∞—Ç (mlx-community)
    ‚úÖ –ú–∞–ª–µ–Ω—å–∫–∏–π —Ä–∞–∑–º–µ—Ä (~150 MB vs 2+ GB)
    ‚úÖ –ë—ã—Å—Ç—Ä–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å
    ‚úÖ –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–ª—è POC
    ‚úÖ –ù–∏–∑–∫–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ RAM (–∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è 8 GB)

–í–´–í–û–î:
    –î–ª—è POC –Ω–∞ 8 GB RAM –≤—ã–±–æ—Ä all-MiniLM –±—ã–ª –ü–†–ê–í–ò–õ–¨–ù–´–ú.
    BGE-M3 –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è production –Ω–∞ 16+ GB, –µ—Å–ª–∏ –ø–æ—è–≤–∏—Ç—Å—è MLX –ø–æ—Ä—Ç.
"""

    print(comparison_table)


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∞."""
    print()
    print("üî¨ –¢–ï–°–¢ –ú–û–î–ï–õ–ò BGE-M3 (–∏–∑ poc_plan.md)")
    print()
    print("–¶–µ–ª—å: –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –ø–æ—á–µ–º—É –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å.")
    print("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ: BGE-M3 vs —Ç–µ–∫—É—â–∞—è all-MiniLM-L6-v2-4bit")
    print()

    # –¢–µ—Å—Ç 1: –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    success, model, tokenizer, model_name = test_model_loading()

    if not success:
        print()
        print("=" * 80)
        print("‚ùå –¢–ï–°–¢ –û–°–¢–ê–ù–û–í–õ–ï–ù: BGE-M3 –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –≤ MLX —Ñ–æ—Ä–º–∞—Ç–µ")
        print("=" * 80)
        print()
        print_final_comparison()
        return

    # –¢–µ—Å—Ç 2: –†—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç
    russian_results = test_russian_text(model, tokenizer, model_name)

    # –¢–µ—Å—Ç 3: –ê–Ω–≥–ª–∏–π—Å–∫–∏–π —Ç–µ–∫—Å—Ç
    english_results = test_english_text(model, tokenizer, model_name)

    # –¢–µ—Å—Ç 4: –ë–ª–æ–∫–∏ –∫–æ–¥–∞
    code_results = test_code_blocks(model, tokenizer, model_name)

    # –ò—Ç–æ–≥–æ–≤–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
    print_final_comparison()

    print()
    print("‚úÖ –í–°–ï –¢–ï–°–¢–´ –ó–ê–í–ï–†–®–ï–ù–´")
    print()


if __name__ == "__main__":
    main()
