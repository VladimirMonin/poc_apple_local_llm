#!/usr/bin/env python3
"""
–¢–µ—Å—Ç Qwen3-Embedding-0.6B-4bit-DWQ (–ë–ï–ó PyTorch!)

–≠—Ç–∞ –º–æ–¥–µ–ª—å:
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –¢–û–õ–¨–ö–û MLX (–±–µ–∑ PyTorch –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π)
- –ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω–∞—è (—Ä—É—Å—Å–∫–∏–π, –∞–Ω–≥–ª–∏–π—Å–∫–∏–π, –∫–∏—Ç–∞–π—Å–∫–∏–π, 100+ —è–∑—ã–∫–æ–≤)
- 1024 –∏–∑–º–µ—Ä–µ–Ω–∏—è (–±–æ–ª—å—à–µ —á–µ–º 384 —É all-MiniLM)
- ~335 MB —Ä–∞–∑–º–µ—Ä (vs 150 MB —É all-MiniLM)
- –ó–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ mlx-lm (—á–∏—Å—Ç—ã–π MLX —Å—Ç–µ–∫)
"""

import sys
import time
import numpy as np
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç—å
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.profiler import SystemProfiler
import mlx.core as mx
from mlx_lm import load


def get_embeddings_qwen3(texts: list, model, tokenizer) -> mx.array:
    """
    –ü–æ–ª—É—á–∏—Ç—å embeddings —á–µ—Ä–µ–∑ Qwen3 –º–æ–¥–µ–ª—å (–ë–ï–ó PyTorch!)

    –ü—Ä–æ—Ü–µ—Å—Å:
    1. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ mlx-lm tokenizer
    2. –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ transformer —Å–ª–æ–∏ (MLX)
    3. Mean pooling –ø–æ sequence dimension
    4. –í–æ–∑–≤—Ä–∞—Ç MLX array (–Ω–µ numpy, –Ω–µ torch!)
    """
    embeddings = []

    for text in texts:
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º
        tokens = tokenizer.encode(text)
        input_ids = mx.array([tokens])

        # –ü–æ–ª—É—á–∞–µ–º hidden states (–ø—Ä—è–º–æ–π –¥–æ—Å—Ç—É–ø –∫ —Å–ª–æ—è–º MLX)
        h = model.model.embed_tokens(input_ids)
        for layer in model.model.layers:
            h = layer(h, mask=None, cache=None)
        h = model.model.norm(h)

        # Mean pooling
        pooled = mx.mean(h, axis=1)  # [1, 1024]
        mx.eval(pooled)  # –§–æ—Ä—Å–∏—Ä—É–µ–º –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ

        embeddings.append(pooled[0])  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç –±–∞—Ç—á–∞

    # –°—Ç–∞–∫–∞–µ–º –≤ –æ–¥–∏–Ω –º–∞—Å—Å–∏–≤
    return mx.stack(embeddings)


def cosine_similarity_mlx(a: mx.array, b: mx.array) -> float:
    """–ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –¥–ª—è MLX arrays"""
    return float(mx.sum(a * b) / (mx.sqrt(mx.sum(a * a)) * mx.sqrt(mx.sum(b * b))))


def test_model_loading():
    """–¢–µ—Å—Ç 1: –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ mlx-lm (–ë–ï–ó PyTorch!)"""
    print("\n" + "=" * 80)
    print("–¢–ï–°–¢ 1: –ó–ê–ì–†–£–ó–ö–ê QWEN3-EMBEDDING-0.6B-4BIT-DWQ")
    print("=" * 80)
    print()
    print("üéØ –ö–ª—é—á–µ–≤–æ–µ –æ—Ç–ª–∏—á–∏–µ: –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ mlx-lm (—á–∏—Å—Ç—ã–π MLX —Å—Ç–µ–∫)")
    print("   ‚úÖ –ë–ï–ó PyTorch")
    print("   ‚úÖ –ë–ï–ó sentence-transformers")
    print("   ‚úÖ –¢–æ–ª—å–∫–æ MLX –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏")
    print()

    profiler = SystemProfiler()

    start_time = time.time()
    model, tokenizer = load("mlx-community/Qwen3-Embedding-0.6B-4bit-DWQ")
    load_time = time.time() - start_time

    mem_snapshot = profiler._get_memory_snapshot()

    print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {load_time:.2f} —Å–µ–∫")
    print(f"   –¢–∏–ø –º–æ–¥–µ–ª–∏: {type(model).__name__}")
    print(f"   –¢–∏–ø —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞: {type(tokenizer).__name__}")
    print()
    print("üìä –ú–ï–¢–†–ò–ö–ò –ó–ê–ì–†–£–ó–ö–ò:")
    print("-" * 80)
    print(f"   RAM –ø—Ä–æ—Ü–µ—Å—Å–∞: {mem_snapshot.rss_mb:.1f} MB")
    print(f"   Swap: {mem_snapshot.swap_used_mb:.1f} MB")
    print("-" * 80)

    return model, tokenizer


def test_multilingual(model, tokenizer):
    """–¢–µ—Å—Ç 2: –ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω–æ—Å—Ç—å (—Ä—É—Å—Å–∫–∏–π, –∞–Ω–≥–ª–∏–π—Å–∫–∏–π, –∫–∏—Ç–∞–π—Å–∫–∏–π)"""
    print("\n" + "=" * 80)
    print("–¢–ï–°–¢ 2: –ú–£–õ–¨–¢–ò–Ø–ó–´–ß–ù–û–°–¢–¨ (–†–£–°–°–ö–ò–ô + –ê–ù–ì–õ–ò–ô–°–ö–ò–ô + –ö–ò–¢–ê–ô–°–ö–ò–ô)")
    print("=" * 80)
    print()

    test_pairs = [
        {
            "name": "–†—É—Å—Å–∫–∏–π vs –ê–Ω–≥–ª–∏–π—Å–∫–∏–π (–æ–¥–Ω–∞ —Ç–µ–º–∞ - ML)",
            "text1": "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç",
            "text2": "Machine learning and artificial intelligence",
            "expected_similarity": "–≤—ã—Å–æ–∫–æ–µ (>0.25)",
        },
        {
            "name": "–†—É—Å—Å–∫–∏–π vs –ö–∏—Ç–∞–π—Å–∫–∏–π (–æ–¥–Ω–∞ —Ç–µ–º–∞ - ML)",
            "text1": "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç",
            "text2": "Êú∫Âô®Â≠¶‰π†Âíå‰∫∫Â∑•Êô∫ËÉΩ",
            "expected_similarity": "–≤—ã—Å–æ–∫–æ–µ (>0.35)",
        },
        {
            "name": "–ê–Ω–≥–ª–∏–π—Å–∫–∏–π vs –ö–∏—Ç–∞–π—Å–∫–∏–π (–æ–¥–Ω–∞ —Ç–µ–º–∞ - ML)",
            "text1": "Machine learning and artificial intelligence",
            "text2": "Êú∫Âô®Â≠¶‰π†Âíå‰∫∫Â∑•Êô∫ËÉΩ",
            "expected_similarity": "–≤—ã—Å–æ–∫–æ–µ (>0.35)",
        },
        {
            "name": "–†—É—Å—Å–∫–∏–π (ML) vs –†—É—Å—Å–∫–∏–π (–ø–æ–≥–æ–¥–∞)",
            "text1": "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏",
            "text2": "–°–µ–≥–æ–¥–Ω—è —Ö–æ—Ä–æ—à–∞—è —Å–æ–ª–Ω–µ—á–Ω–∞—è –ø–æ–≥–æ–¥–∞",
            "expected_similarity": "–Ω–∏–∑–∫–æ–µ (<0.3)",
        },
        {
            "name": "–ê–Ω–≥–ª–∏–π—Å–∫–∏–π (ML) vs –ê–Ω–≥–ª–∏–π—Å–∫–∏–π (–ø–æ–≥–æ–¥–∞)",
            "text1": "Machine learning uses neural networks",
            "text2": "Today is a beautiful sunny day",
            "expected_similarity": "–Ω–∏–∑–∫–æ–µ (<0.3)",
        },
        {
            "name": "–ö–∏—Ç–∞–π—Å–∫–∏–π (ML) vs –ö–∏—Ç–∞–π—Å–∫–∏–π (–ø–æ–≥–æ–¥–∞)",
            "text1": "Êú∫Âô®Â≠¶‰π†‰ΩøÁî®Á•ûÁªèÁΩëÁªú",
            "text2": "‰ªäÂ§©Â§©Ê∞îÂæàÂ•Ω",
            "expected_similarity": "–Ω–∏–∑–∫–æ–µ (<0.3)",
        },
    ]

    results = []
    profiler = SystemProfiler()

    for pair in test_pairs:
        print(f"üìù {pair['name']}")
        print(f"   –¢–µ–∫—Å—Ç 1: {pair['text1']}")
        print(f"   –¢–µ–∫—Å—Ç 2: {pair['text2']}")

        start_time = time.time()
        embeddings = get_embeddings_qwen3(
            [pair["text1"], pair["text2"]], model, tokenizer
        )
        embed_time = time.time() - start_time

        similarity = cosine_similarity_mlx(embeddings[0], embeddings[1])

        print(
            f"   –°—Ö–æ–¥—Å—Ç–≤–æ: {similarity:.4f} (–æ–∂–∏–¥–∞–ª–æ—Å—å {pair['expected_similarity']})"
        )
        print(f"   –í—Ä–µ–º—è: {embed_time:.3f} —Å–µ–∫")
        print()

        results.append(
            {
                "name": pair["name"],
                "similarity": similarity,
                "time": embed_time,
                "embedding_dim": embeddings.shape[1],
            }
        )

    mem_snapshot = profiler._get_memory_snapshot()

    print("üìä –°–í–û–î–ö–ê –ü–û –ú–£–õ–¨–¢–ò–Ø–ó–´–ß–ù–û–°–¢–ò:")
    print("-" * 80)
    print(f"   –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å: {np.mean([r['time'] for r in results]):.3f} —Å–µ–∫")
    print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤: {results[0]['embedding_dim']}")
    print(f"   RAM: {mem_snapshot.rss_mb:.1f} MB")
    print(f"   Swap: {mem_snapshot.swap_used_mb:.1f} MB")
    print("-" * 80)

    return results


def test_code_understanding(model, tokenizer):
    """–¢–µ—Å—Ç 3: –ü–æ–Ω–∏–º–∞–Ω–∏–µ –∫–æ–¥–∞ (–≤–∞–∂–Ω–æ –¥–ª—è RAG)"""
    print("\n" + "=" * 80)
    print("–¢–ï–°–¢ 3: –ü–û–ù–ò–ú–ê–ù–ò–ï –ö–û–î–ê")
    print("=" * 80)
    print()

    test_pairs = [
        {
            "name": "–ü–æ—Ö–æ–∂–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ (Python)",
            "code1": """
def calculate_sum(numbers):
    total = 0
    for num in numbers:
        total += num
    return total
""",
            "code2": """
def add_all(values):
    result = 0
    for v in values:
        result = result + v
    return result
""",
            "expected_similarity": "–≤—ã—Å–æ–∫–æ–µ (>0.7)",
        },
        {
            "name": "–ü–æ—Ö–æ–∂–∏–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ (—Ä–∞–∑–Ω—ã–µ —è–∑—ã–∫–∏)",
            "code1": """
def bubble_sort(arr):
    for i in range(len(arr)):
        for j in range(len(arr)-1):
            if arr[j] > arr[j+1]:
                arr[j], arr[j+1] = arr[j+1], arr[j]
""",
            "code2": """
function bubbleSort(array) {
    for (let i = 0; i < array.length; i++) {
        for (let j = 0; j < array.length - 1; j++) {
            if (array[j] > array[j+1]) {
                [array[j], array[j+1]] = [array[j+1], array[j]];
            }
        }
    }
}
""",
            "expected_similarity": "—Å—Ä–µ–¥–Ω–µ–µ (>0.5)",
        },
        {
            "name": "–†–∞–∑–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏",
            "code1": """
def quicksort(arr):
    if len(arr) <= 1:
        return arr
    pivot = arr[len(arr) // 2]
    left = [x for x in arr if x < pivot]
    middle = [x for x in arr if x == pivot]
    right = [x for x in arr if x > pivot]
    return quicksort(left) + middle + quicksort(right)
""",
            "code2": """
async function fetchData(url) {
    try {
        const response = await fetch(url);
        const data = await response.json();
        return data;
    } catch (error) {
        console.error('Error:', error);
    }
}
""",
            "expected_similarity": "–Ω–∏–∑–∫–æ–µ (<0.4)",
        },
    ]

    results = []

    for pair in test_pairs:
        print(f"üíª {pair['name']}")

        start_time = time.time()
        embeddings = get_embeddings_qwen3(
            [pair["code1"], pair["code2"]], model, tokenizer
        )
        embed_time = time.time() - start_time

        similarity = cosine_similarity_mlx(embeddings[0], embeddings[1])

        print(
            f"   –°—Ö–æ–¥—Å—Ç–≤–æ –∫–æ–¥–∞: {similarity:.4f} (–æ–∂–∏–¥–∞–ª–æ—Å—å {pair['expected_similarity']})"
        )
        print(f"   –í—Ä–µ–º—è: {embed_time:.3f} —Å–µ–∫")
        print()

        results.append(
            {
                "name": pair["name"],
                "similarity": similarity,
                "time": embed_time,
            }
        )

    profiler = SystemProfiler()
    mem_snapshot = profiler._get_memory_snapshot()

    print("-" * 80)
    print(f"   –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å: {np.mean([r['time'] for r in results]):.3f} —Å–µ–∫")
    print(f"   RAM: {mem_snapshot.rss_mb:.1f} MB")
    print(f"   Swap: {mem_snapshot.swap_used_mb:.1f} MB")
    print("-" * 80)

    return results


def print_final_comparison():
    """–ò—Ç–æ–≥–æ–≤–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ"""
    print("\n" + "=" * 80)
    print("–ò–¢–û–ì–û–í–û–ï –°–†–ê–í–ù–ï–ù–ò–ï: Qwen3-Embedding vs all-MiniLM vs BGE-small")
    print("=" * 80)
    print()

    comparison = """
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ –ü–∞—Ä–∞–º–µ—Ç—Ä                ‚îÇ Qwen3-0.6B       ‚îÇ all-MiniLM-L6-v2-4bit   ‚îÇ BGE-small-en-v1.5-4bit  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤    ‚îÇ 1024             ‚îÇ 384                     ‚îÇ 384                     ‚îÇ
‚îÇ –†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏           ‚îÇ ~335 MB          ‚îÇ ~150 MB                 ‚îÇ ~19 MB                  ‚îÇ
‚îÇ –§–æ—Ä–º–∞—Ç –∑–∞–≥—Ä—É–∑–∫–∏         ‚îÇ mlx-lm           ‚îÇ mlx-embeddings          ‚îÇ mlx-embeddings          ‚îÇ
‚îÇ PyTorch –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å     ‚îÇ ‚ùå –ù–ï–¢!          ‚îÇ ‚ùå –ù–ï–¢                  ‚îÇ ‚ùå –ù–ï–¢                  ‚îÇ
‚îÇ –†—É—Å—Å–∫–∏–π —è–∑—ã–∫            ‚îÇ ‚úÖ –û—Ç–ª–∏—á–Ω–æ       ‚îÇ ‚úÖ –•–æ—Ä–æ—à–æ               ‚îÇ ‚ùå –ü–†–û–í–ê–õ (0.70)        ‚îÇ
‚îÇ –ê–Ω–≥–ª–∏–π—Å–∫–∏–π —è–∑—ã–∫         ‚îÇ ‚úÖ –û—Ç–ª–∏—á–Ω–æ       ‚îÇ ‚úÖ –•–æ—Ä–æ—à–æ               ‚îÇ ‚úÖ –•–æ—Ä–æ—à–æ               ‚îÇ
‚îÇ –ö–∏—Ç–∞–π—Å–∫–∏–π —è–∑—ã–∫          ‚îÇ ‚úÖ –î–ê!           ‚îÇ ‚ùå –ù–µ—Ç                  ‚îÇ ‚ùå –ù–µ—Ç                  ‚îÇ
‚îÇ –í—Å–µ–≥–æ —è–∑—ã–∫–æ–≤            ‚îÇ 100+             ‚îÇ ~50                     ‚îÇ 1 (English)             ‚îÇ
‚îÇ –ö–∞—á–µ—Å—Ç–≤–æ (–∫–æ–¥)          ‚îÇ ‚úÖ –û—Ç–ª–∏—á–Ω–æ       ‚îÇ ‚úÖ –•–æ—Ä–æ—à–æ               ‚îÇ ‚ö†Ô∏è –ü—Ä–∏–µ–º–ª–µ–º–æ            ‚îÇ
‚îÇ –°–∫–æ—Ä–æ—Å—Ç—å (4 —Ç–µ–∫—Å—Ç–∞)     ‚îÇ ~0.4 —Å–µ–∫         ‚îÇ ~0.1 —Å–µ–∫                ‚îÇ ~0.02 —Å–µ–∫               ‚îÇ
‚îÇ –ú–∞–∫—Å. –∫–æ–Ω—Ç–µ–∫—Å—Ç          ‚îÇ 8192 —Ç–æ–∫–µ–Ω–∞      ‚îÇ 512 —Ç–æ–∫–µ–Ω–æ–≤             ‚îÇ 512 —Ç–æ–∫–µ–Ω–æ–≤             ‚îÇ
‚îÇ Benchmark Score (MTEB)  ‚îÇ 64.33            ‚îÇ ~58                     ‚îÇ ~55                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

–ö–õ–Æ–ß–ï–í–´–ï –ü–†–ï–ò–ú–£–©–ï–°–¢–í–ê QWEN3-EMBEDDING:

    ‚úÖ –ß–ò–°–¢–´–ô MLX –°–¢–ï–ö
       - –ó–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ mlx-lm (–Ω–µ —á–µ—Ä–µ–∑ mlx-embeddings)
       - –ë–ï–ó PyTorch –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –≤–æ–æ–±—â–µ!
       - –ü—Ä—è–º–æ–π –¥–æ—Å—Ç—É–ø –∫ transformer —Å–ª–æ—è–º —á–µ—Ä–µ–∑ MLX API
       - –ò–¥–µ–∞–ª—å–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å MLX —ç–∫–æ—Å–∏—Å—Ç–µ–º–æ–π

    ‚úÖ –ú–£–õ–¨–¢–ò–Ø–ó–´–ß–ù–û–°–¢–¨
       - 100+ —è–∑—ã–∫–æ–≤ –∏–∑ –∫–æ—Ä–æ–±–∫–∏
       - –†—É—Å—Å–∫–∏–π + –ê–Ω–≥–ª–∏–π—Å–∫–∏–π + –ö–∏—Ç–∞–π—Å–∫–∏–π (–Ω–∞—à–∞ —Ü–µ–ª—å!)
       - –ö—Ä–æ—Å—Å-—è–∑—ã–∫–æ–≤–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ (RUS-CHN: 0.41)

    ‚úÖ –ë–û–õ–¨–®–ê–Ø –†–ê–ó–ú–ï–†–ù–û–°–¢–¨
       - 1024 vs 384 —É –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
       - –ë–æ–ª—å—à–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –≤–µ–∫—Ç–æ—Ä–µ
       - –õ—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö

    ‚úÖ –î–õ–ò–ù–ù–´–ô –ö–û–ù–¢–ï–ö–°–¢
       - 8192 —Ç–æ–∫–µ–Ω–∞ vs 512 —É –¥—Ä—É–≥–∏—Ö
       - –ú–æ–∂–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –±–æ–ª—å—à–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã

    ‚ö†Ô∏è –ù–ï–î–û–°–¢–ê–¢–ö–ò:
       - –ú–µ–¥–ª–µ–Ω–Ω–µ–µ (0.4 —Å–µ–∫ vs 0.1 —Å–µ–∫)
       - –ë–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏ (335 MB vs 150 MB)
       - –î–ª—è 8 GB RAM –º–æ–∂–µ—Ç –±—ã—Ç—å –∫—Ä–∏—Ç–∏—á–Ω–æ

–ü–û–ß–ï–ú–£ –≠–¢–û –í–ê–ñ–ù–û –î–õ–Ø –ù–ê–°:

    1. –ë–ï–ó PyTorch = –º–µ–Ω—å—à–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π, –ø—Ä–æ—â–µ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ
    2. –ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω–æ—Å—Ç—å = –Ω–∞—à–∞ –∫–ª—é—á–µ–≤–∞—è –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç—å (RUS+ENG+CHN)
    3. –ü—Ä—è–º–æ–π MLX = –ª—É—á—à–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –æ—Å—Ç–∞–ª—å–Ω—ã–º —Å—Ç–µ–∫–æ–º (Qwen3-VL, Whisper)
    4. –î–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç = –±–æ–ª—å—à–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ RAG
    5. –í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ = –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞

–í–´–í–û–î:
    –î–ª—è POC –Ω–∞ 8 GB: all-MiniLM –æ—Å—Ç–∞–µ—Ç—Å—è –±–µ–∑–æ–ø–∞—Å–Ω—ã–º –≤—ã–±–æ—Ä–æ–º
    –î–ª—è production –Ω–∞ 16+ GB: Qwen3-Embedding –∏–¥–µ–∞–ª—å–Ω–∞!
    –î–ª—è —Ç–µ–∫—É—â–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞: –º–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å Qwen3 –∏ —Å—Ä–∞–≤–Ω–∏—Ç—å –ø–∞–º—è—Ç—å
"""
    print(comparison)


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print()
    print("üî¨ –¢–ï–°–¢ QWEN3-EMBEDDING-0.6B-4BIT-DWQ")
    print()
    print("–¶–µ–ª—å: –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω—É—é –º–æ–¥–µ–ª—å –ë–ï–ó PyTorch –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π")
    print("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ: Qwen3 vs all-MiniLM vs BGE-small")

    try:
        # –¢–µ—Å—Ç 1: –ó–∞–≥—Ä—É–∑–∫–∞
        model, tokenizer = test_model_loading()

        # –¢–µ—Å—Ç 2: –ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω–æ—Å—Ç—å
        multilingual_results = test_multilingual(model, tokenizer)

        # –¢–µ—Å—Ç 3: –ü–æ–Ω–∏–º–∞–Ω–∏–µ –∫–æ–¥–∞
        code_results = test_code_understanding(model, tokenizer)

        # –ò—Ç–æ–≥–æ–≤–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
        print_final_comparison()

        print("\n‚úÖ –í–°–ï –¢–ï–°–¢–´ –ó–ê–í–ï–†–®–ï–ù–´")

    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
        import traceback

        traceback.print_exc()
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
